\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{biblatex}
\bibdata{main-blx,main}
\citation{biblatex-control}
\abx@aux@refcontext{nty/global//global/global/global}
\providecommand \oddpage@label [2]{}
\newlabel{sec:abstract}{{}{2}{}{Doc-Start}{}}
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{top_signatures}
\abx@aux@cite{0}{top_signatures}
\abx@aux@segm{0}{0}{top_signatures}
\citation{optimizing_persistent_homology}
\abx@aux@cite{0}{optimizing_persistent_homology}
\abx@aux@segm{0}{0}{optimizing_persistent_homology}
\citation{persistent_homology_seg}
\abx@aux@cite{0}{persistent_homology_seg}
\abx@aux@segm{0}{0}{persistent_homology_seg}
\citation{atol}
\abx@aux@cite{0}{atol}
\abx@aux@segm{0}{0}{atol}
\citation{neural_approximation_graph_topological_features}
\abx@aux@cite{0}{neural_approximation_graph_topological_features}
\abx@aux@segm{0}{0}{neural_approximation_graph_topological_features}
\citation{topological_autoencoders}
\abx@aux@cite{0}{topological_autoencoders}
\abx@aux@segm{0}{0}{topological_autoencoders}
\citation{top_layer}
\abx@aux@cite{0}{top_layer}
\abx@aux@segm{0}{0}{top_layer}
\citation{topology_activations}
\abx@aux@cite{0}{topology_activations}
\abx@aux@segm{0}{0}{topology_activations}
\citation{top_reg}
\abx@aux@cite{0}{top_reg}
\abx@aux@segm{0}{0}{top_reg}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{3}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Current State of Research}{3}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Motivations}{3}{subsection.1.2}\protected@file@percent }
\citation{objaverse}
\abx@aux@cite{0}{objaverse}
\abx@aux@segm{0}{0}{objaverse}
\citation{objaverse_xl}
\abx@aux@cite{0}{objaverse_xl}
\abx@aux@segm{0}{0}{objaverse_xl}
\citation{shapenet}
\abx@aux@cite{0}{shapenet}
\abx@aux@segm{0}{0}{shapenet}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:topogen-samples}{{\caption@xref {fig:topogen-samples}{ on input line 8}}{4}{DONUT: \underline {D}ataset \underline {O}f ma\underline {N}ifold str\underline {U}c\underline {T}ures}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Random samples from DONUT.} Despite the fact that each sample is generated from a rather small family of shapes, both the topology preserving placement and augmentations allow for a rich variety of shapes, while ensuring topological consistency.}}{4}{figure.caption.1}\protected@file@percent }
\newlabel{fig:short}{{1}{4}{\textbf {Random samples from DONUT.} Despite the fact that each sample is generated from a rather small family of shapes, both the topology preserving placement and augmentations allow for a rich variety of shapes, while ensuring topological consistency}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contributions}{4}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}DONUT: \underline  {D}ataset \underline  {O}f ma\underline  {N}ifold str\underline  {U}c\underline  {T}ures}{4}{section.2}\protected@file@percent }
\newlabel{sec:topogen}{{2}{4}{DONUT: \underline {D}ataset \underline {O}f ma\underline {N}ifold str\underline {U}c\underline {T}ures}{section.2}{}}
\citation{mantra}
\abx@aux@cite{0}{mantra}
\abx@aux@segm{0}{0}{mantra}
\citation{mantra}
\abx@aux@cite{0}{mantra}
\abx@aux@segm{0}{0}{mantra}
\citation{abc}
\abx@aux@cite{0}{abc}
\abx@aux@segm{0}{0}{abc}
\citation{thingi}
\abx@aux@cite{0}{thingi}
\abx@aux@segm{0}{0}{thingi}
\citation{eulearn}
\abx@aux@cite{0}{eulearn}
\abx@aux@segm{0}{0}{eulearn}
\citation{thingi}
\abx@aux@cite{0}{thingi}
\abx@aux@segm{0}{0}{thingi}
\citation{objaverse}
\abx@aux@cite{0}{objaverse}
\abx@aux@segm{0}{0}{objaverse}
\citation{abc}
\abx@aux@cite{0}{abc}
\abx@aux@segm{0}{0}{abc}
\citation{manifold}
\abx@aux@cite{0}{manifold}
\abx@aux@segm{0}{0}{manifold}
\citation{manifoldplus}
\abx@aux@cite{0}{manifoldplus}
\abx@aux@segm{0}{0}{manifoldplus}
\citation{dogn}
\abx@aux@cite{0}{dogn}
\abx@aux@segm{0}{0}{dogn}
\citation{clay}
\abx@aux@cite{0}{clay}
\abx@aux@segm{0}{0}{clay}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Existing datasets}{5}{subsection.2.1}\protected@file@percent }
\newlabel{ssec:existing_datasets}{{2.1}{5}{Existing datasets}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Combinatorial benchmarks.}{5}{subsection.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Overview of existing datasets and their capabilities.} We summarize here the main characteristics of existing datasets with topological annotations. Besides EuLearn, all existing datasets with topological annotations come with downsides, discussed in Section~\ref {ssec:existing_datasets}. However, since EuLearn seems to be the most promising dataset, we carried out an extensive analysis to (1) highlight limitations that make it unreliable for further experiments and (2) motivate the use of DONUT (see Appendix~\ref {ssec:suppl_eulearn_analysis}). \textit  {Note:} The number of samples for MANTRA only takes into account 2-manifolds.}}{5}{table.caption.2}\protected@file@percent }
\newlabel{tab:datasets}{{1}{5}{\textbf {Overview of existing datasets and their capabilities.} We summarize here the main characteristics of existing datasets with topological annotations. Besides EuLearn, all existing datasets with topological annotations come with downsides, discussed in Section~\ref {ssec:existing_datasets}. However, since EuLearn seems to be the most promising dataset, we carried out an extensive analysis to (1) highlight limitations that make it unreliable for further experiments and (2) motivate the use of DONUT (see Appendix~\ref {ssec:suppl_eulearn_analysis}). \textit {Note:} The number of samples for MANTRA only takes into account 2-manifolds}{table.caption.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Geometric datasets with noisy topology.}{5}{table.caption.2}\protected@file@percent }
\newlabel{eq:euler}{{1}{5}{Geometric datasets with noisy topology}{equation.1}{}}
\citation{eulearn}
\abx@aux@cite{0}{eulearn}
\abx@aux@segm{0}{0}{eulearn}
\newlabel{fig:thingi-genus}{{2a}{6}{\textbf {Genus distribution of Thingi10K.} Both axes are in log scale. The genus, estimated from the Euler characteristic provided as metadata, could not be computed for 2,651 samples, which are excluded. The histogram shows that most meshes (over 3,000 of 7,344) have genus 0 or 1, highlighting the scarcity of higher-genus shapes}{figure.caption.3}{}}
\newlabel{sub@fig:thingi-genus}{{a}{6}{\textbf {Genus distribution of Thingi10K.} Both axes are in log scale. The genus, estimated from the Euler characteristic provided as metadata, could not be computed for 2,651 samples, which are excluded. The histogram shows that most meshes (over 3,000 of 7,344) have genus 0 or 1, highlighting the scarcity of higher-genus shapes}{figure.caption.3}{}}
\newlabel{fig:thingi-comps}{{2b}{6}{\textbf {Raw connected components.} The left figure shows a mesh made of 8 connected components, highlighted with different colors. However, once converted into a point cloud (right figure), we lose track of this information. Therefore, labeling this sample as having 8 connected components would be misleading for a model}{figure.caption.3}{}}
\newlabel{sub@fig:thingi-comps}{{b}{6}{\textbf {Raw connected components.} The left figure shows a mesh made of 8 connected components, highlighted with different colors. However, once converted into a point cloud (right figure), we lose track of this information. Therefore, labeling this sample as having 8 connected components would be misleading for a model}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:thingi-overview}{{2}{6}{\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Synthetic datasets with limited diversity.}{6}{equation.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Samples from the EuLearn dataset across different genera.} As the genus increases, shapes become geometrically more complex. This trend highlights a confounding factor in the dataset: geometric complexity grows together with genus. As a result, classification performance may be driven not only by topological information but also by correlated geometric cues.}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:eulearn-samples}{{3}{6}{\textbf {Samples from the EuLearn dataset across different genera.} As the genus increases, shapes become geometrically more complex. This trend highlights a confounding factor in the dataset: geometric complexity grows together with genus. As a result, classification performance may be driven not only by topological information but also by correlated geometric cues}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Method}{6}{subsection.2.2}\protected@file@percent }
\newlabel{ssec:topogen-method}{{2.2}{6}{Method}{subsection.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Generation Process Overview.} (1) For each sample, we pick a target genus and number of connected components. (Section~\ref {sssec:labels-distribution}) (2) We then distribute these values across a set of template shapes (Section~\ref {sssec:sample-level-properties}). (3) Each mesh is instantiated by sampling parameters from its parametric expression, and individually warped to bring more geometric diversity. (4) Finally, we apply a series of topology-preserving augmentations to increase geometric diversity while maintaining the original topological labels, and compose the final sample by merging all meshes. (Section~\ref {sssec:mesh-generation})}}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig:topogen-overview}{{4}{7}{\textbf {Generation Process Overview.} (1) For each sample, we pick a target genus and number of connected components. (Section~\ref {sssec:labels-distribution}) (2) We then distribute these values across a set of template shapes (Section~\ref {sssec:sample-level-properties}). (3) Each mesh is instantiated by sampling parameters from its parametric expression, and individually warped to bring more geometric diversity. (4) Finally, we apply a series of topology-preserving augmentations to increase geometric diversity while maintaining the original topological labels, and compose the final sample by merging all meshes. (Section~\ref {sssec:mesh-generation})}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {UMAP embeddings of features learned by 3D encoders.} DONUT samples project into the same space as the original training data, indicating that they are not out-of-distribution.}}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:topogen-umaps-overview}{{5}{7}{\textbf {UMAP embeddings of features learned by 3D encoders.} DONUT samples project into the same space as the original training data, indicating that they are not out-of-distribution}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Topological Control}{7}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{sssec:labels-distribution}{{2.2.1}{7}{Topological Control}{subsubsection.2.2.1}{}}
\citation{superquadrics}
\abx@aux@cite{0}{superquadrics}
\abx@aux@segm{0}{0}{superquadrics}
\citation{superdec}
\abx@aux@cite{0}{superdec}
\abx@aux@segm{0}{0}{superdec}
\citation{pmaezero}
\abx@aux@cite{0}{pmaezero}
\abx@aux@segm{0}{0}{pmaezero}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Geometric Diversity}{8}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{sssec:sample-level-properties}{{2.2.2}{8}{Geometric Diversity}{subsubsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Scalability}{8}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{sssec:mesh-generation}{{2.2.3}{8}{Scalability}{subsubsection.2.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Cones.}{8}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Superquadrics.}{8}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{fig:toroids-overview}{{6a}{9}{Supertoroids}{figure.caption.7}{}}
\newlabel{sub@fig:toroids-overview}{{a}{9}{Supertoroids}{figure.caption.7}{}}
\newlabel{fig:ellipsoids-overview}{{6b}{9}{Superellipsoids}{figure.caption.7}{}}
\newlabel{sub@fig:ellipsoids-overview}{{b}{9}{Superellipsoids}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Overview of different shapes obtained for fixed values of $a_i, i \in \{1, 2, 3\}$ and increasing values of $\epsilon _1$ (left to right) and $\epsilon _2$ (bottom to top). (a) Different supertoroids. As mentioned in TO ADD, using these shapes for $k$-tori ($k \geq 2$) is challenging because they may not preserve the genus, for instance if some parts are too thin or sharp. (b) Different superellipsoids.}}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:overview}{{6}{9}{Overview of different shapes obtained for fixed values of $a_i, i \in \{1, 2, 3\}$ and increasing values of $\epsilon _1$ (left to right) and $\epsilon _2$ (bottom to top). (a) Different supertoroids. As mentioned in TO ADD, using these shapes for $k$-tori ($k \geq 2$) is challenging because they may not preserve the genus, for instance if some parts are too thin or sharp. (b) Different superellipsoids}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{$K$-tori.}{9}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Topological Consistency and Diversity}{9}{subsubsection.2.2.4}\protected@file@percent }
\newlabel{sssec:top-consistency}{{2.2.4}{9}{Topological Consistency and Diversity}{subsubsection.2.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Consistency.}{10}{subsubsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variability.}{10}{subsubsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Analysis}{10}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}General Properties}{10}{subsubsection.2.3.1}\protected@file@percent }
\newlabel{sssec:topogen-general-properties}{{2.3.1}{10}{General Properties}{subsubsection.2.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Dataset statistics.}{10}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Distributions and scalability.}{10}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Baselines and Transferability}{10}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{sssec:topogen-transferability}{{2.3.2}{10}{Baselines and Transferability}{subsubsection.2.3.2}{}}
\newlabel{fig:ktori-overview}{{7a}{11}{\textbf {Degrees of freedom allowed on $k$-tori.} Prior to the sequence of transformations applied to each individual component (e.g. rotation, twisting) we allow some variability during the generation. The x-axis represents the ratio \textit {major radius / minor radius}. The y-axis shows how the template shape is modified as we increase the value of $k$ (i.e. the genus). \textit {Note:} In the actual dataset, $1$-tori are generated with the parametric representation of supertoroids, since a regular torus is a particular case of supertoroid. However, for $k \geq 2$, using a composition of $1$-tori is more reliable. Some parameter sets can lead to undesirable holes in the final shape}{figure.caption.8}{}}
\newlabel{sub@fig:ktori-overview}{{a}{11}{\textbf {Degrees of freedom allowed on $k$-tori.} Prior to the sequence of transformations applied to each individual component (e.g. rotation, twisting) we allow some variability during the generation. The x-axis represents the ratio \textit {major radius / minor radius}. The y-axis shows how the template shape is modified as we increase the value of $k$ (i.e. the genus). \textit {Note:} In the actual dataset, $1$-tori are generated with the parametric representation of supertoroids, since a regular torus is a particular case of supertoroid. However, for $k \geq 2$, using a composition of $1$-tori is more reliable. Some parameter sets can lead to undesirable holes in the final shape}{figure.caption.8}{}}
\newlabel{fig:twisted-comparison}{{7b}{11}{\textbf {Effect of twisting.} Original template shapes (left) are twisted along the purple axis (right). Twisting deformations introduce non-rigid variability while preserving the genus and connectivity of each component. Here the scalar function defined along the axis is affine between $-\pi /6$ and $\pi /3$. We apply this augmentation at the component and sample level}{figure.caption.8}{}}
\newlabel{sub@fig:twisted-comparison}{{b}{11}{\textbf {Effect of twisting.} Original template shapes (left) are twisted along the purple axis (right). Twisting deformations introduce non-rigid variability while preserving the genus and connectivity of each component. Here the scalar function defined along the axis is affine between $-\pi /6$ and $\pi /3$. We apply this augmentation at the component and sample level}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Label distributions for genus and connected components.} The marginal distributions of genus and connected components (left and middle histograms) are approximately uniform, ensuring balanced supervision across topological classes. The right plot shows the joint distribution. The absence of samples with one connected component and genus greater than 5 stems from the constraints of our generation process, which limits individual components to a maximum genus of 5. The only way to achieve a genus greater than 5 is by combining multiple components.}}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:topogen-properties}{{8}{11}{\textbf {Label distributions for genus and connected components.} The marginal distributions of genus and connected components (left and middle histograms) are approximately uniform, ensuring balanced supervision across topological classes. The right plot shows the joint distribution. The absence of samples with one connected component and genus greater than 5 stems from the constraints of our generation process, which limits individual components to a maximum genus of 5. The only way to achieve a genus greater than 5 is by combining multiple components}{figure.caption.9}{}}
\citation{pointnet}
\abx@aux@cite{0}{pointnet}
\abx@aux@segm{0}{0}{pointnet}
\citation{pointnet++}
\abx@aux@cite{0}{pointnet++}
\abx@aux@segm{0}{0}{pointnet++}
\citation{dgcnn}
\abx@aux@cite{0}{dgcnn}
\abx@aux@segm{0}{0}{dgcnn}
\citation{persformer}
\abx@aux@cite{0}{persformer}
\abx@aux@segm{0}{0}{persformer}
\citation{xpert}
\abx@aux@cite{0}{xpert}
\abx@aux@segm{0}{0}{xpert}
\citation{perslay}
\abx@aux@cite{0}{perslay}
\abx@aux@segm{0}{0}{perslay}
\citation{superformula}
\abx@aux@cite{0}{superformula}
\abx@aux@segm{0}{0}{superformula}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Performance of baseline models trained on DONUT and evaluated both in-distribution and on ABC.} Values are reported as \textit  {DONUT/ABC}. We report mean squared error (MSE), balanced accuracy (Acc.), and balanced F1-score (F1). MSE is reported to capture how far off predictions are from the ground-truth on average, which is particularly relevant given the natural hierarchy of the labels (genus and connected components): misclassifying by a larger margin is more severe than a closer miss. }}{12}{table.caption.10}\protected@file@percent }
\newlabel{tab:topogen-results}{{2}{12}{\textbf {Performance of baseline models trained on DONUT and evaluated both in-distribution and on ABC.} Values are reported as \textit {DONUT/ABC}. We report mean squared error (MSE), balanced accuracy (Acc.), and balanced F1-score (F1). MSE is reported to capture how far off predictions are from the ground-truth on average, which is particularly relevant given the natural hierarchy of the labels (genus and connected components): misclassifying by a larger margin is more severe than a closer miss}{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Saliency Analysis}{12}{subsubsection.2.3.3}\protected@file@percent }
\newlabel{sssec:topogen-saliency}{{2.3.3}{12}{Saliency Analysis}{subsubsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Limitations and Further Improvements}{12}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Topological Variety.}{12}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometric Complexity.}{12}{subsection.2.4}\protected@file@percent }
\citation{set-transformer}
\abx@aux@cite{0}{set-transformer}
\abx@aux@segm{0}{0}{set-transformer}
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{pm2ae}
\abx@aux@cite{0}{pm2ae}
\abx@aux@segm{0}{0}{pm2ae}
\citation{pcpmae}
\abx@aux@cite{0}{pcpmae}
\abx@aux@segm{0}{0}{pcpmae}
\citation{pointnet}
\abx@aux@cite{0}{pointnet}
\abx@aux@segm{0}{0}{pointnet}
\citation{pointnet++}
\abx@aux@cite{0}{pointnet++}
\abx@aux@segm{0}{0}{pointnet++}
\citation{dgcnn}
\abx@aux@cite{0}{dgcnn}
\abx@aux@segm{0}{0}{dgcnn}
\@writefile{toc}{\contentsline {section}{\numberline {3}Used Models}{13}{section.3}\protected@file@percent }
\newlabel{sec:used_models}{{3}{13}{Used Models}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Baselines Models}{13}{subsection.3.1}\protected@file@percent }
\newlabel{ssec:baselines_models}{{3.1}{13}{Baselines Models}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}PointNet}{13}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}PointNet++}{13}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}DGCNN (Dynamic Graph CNN)}{13}{subsubsection.3.1.3}\protected@file@percent }
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Transformer Overview for 3D point-clouds.} A point cloud is first partitioned into local patches using farthest point sampling (FPS) and K nearest neighbors (KNN). Patches are then embedded and augmented with positional encodings. The resulting sequence of patch embeddings is processed by a Transformer encoder to produce a global shape representation.}}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:transformer-overview}{{9}{14}{\textbf {Transformer Overview for 3D point-clouds.} A point cloud is first partitioned into local patches using farthest point sampling (FPS) and K nearest neighbors (KNN). Patches are then embedded and augmented with positional encodings. The resulting sequence of patch embeddings is processed by a Transformer encoder to produce a global shape representation}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Transformer-based models}{14}{subsection.3.2}\protected@file@percent }
\newlabel{ssec:transformer_based_models}{{3.2}{14}{Transformer-based models}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Point-BERT: Discrete Tokenization and Masked Modeling}{14}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{sssec:pointbert}{{3.2.1}{14}{Point-BERT: Discrete Tokenization and Masked Modeling}{subsubsection.3.2.1}{}}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{mae}
\abx@aux@cite{0}{mae}
\abx@aux@segm{0}{0}{mae}
\citation{pm2ae}
\abx@aux@cite{0}{pm2ae}
\abx@aux@segm{0}{0}{pm2ae}
\citation{pcpmae}
\abx@aux@cite{0}{pcpmae}
\abx@aux@segm{0}{0}{pcpmae}
\citation{hfbrimae}
\abx@aux@cite{0}{hfbrimae}
\abx@aux@segm{0}{0}{hfbrimae}
\citation{prae}
\abx@aux@cite{0}{prae}
\abx@aux@segm{0}{0}{prae}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Point-MAE: Continuous Tokens and Masked Autoencoding}{15}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Hierarchical Extensions: Multi-Scale Representations}{15}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{sssec:hierarchical_extensions}{{3.2.3}{15}{Hierarchical Extensions: Multi-Scale Representations}{subsubsection.3.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Further Improvements}{15}{subsubsection.3.2.4}\protected@file@percent }
\newlabel{sssec:further_improvements}{{3.2.4}{15}{Further Improvements}{subsubsection.3.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Comparative View}{15}{subsubsection.3.2.5}\protected@file@percent }
\newlabel{sssec:comparative_view}{{3.2.5}{15}{Comparative View}{subsubsection.3.2.5}{}}
\citation{plato}
\abx@aux@cite{0}{plato}
\abx@aux@segm{0}{0}{plato}
\citation{platonic}
\abx@aux@cite{0}{platonic}
\abx@aux@segm{0}{0}{platonic}
\citation{platonic}
\abx@aux@cite{0}{platonic}
\abx@aux@segm{0}{0}{platonic}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{16}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{16}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Probing}{16}{subsection.4.1}\protected@file@percent }
\newlabel{ssec:model_probing}{{4.1}{16}{Model Probing}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Subspace Alignment with Persistent Homology}{16}{subsection.4.2}\protected@file@percent }
\newlabel{ssec:ph}{{4.2}{16}{Subspace Alignment with Persistent Homology}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Persistence Diagrams}{16}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{sssec:persistence_diagrams}{{4.2.1}{16}{Persistence Diagrams}{subsubsection.4.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Filtrations.}{16}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Homology and persistent maps.}{16}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Persistence modules, barcodes and diagrams.}{16}{equation.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why a persistence diagram is not a Euclidean vector object.}{16}{equation.6}\protected@file@percent }
\citation{perslay}
\abx@aux@cite{0}{perslay}
\abx@aux@segm{0}{0}{perslay}
\citation{perslay}
\abx@aux@cite{0}{perslay}
\abx@aux@segm{0}{0}{perslay}
\newlabel{fig:filtrations_top}{{10a}{17}{Sub- (resp. super-) level set filtration}{figure.caption.12}{}}
\newlabel{sub@fig:filtrations_top}{{a}{17}{Sub- (resp. super-) level set filtration}{figure.caption.12}{}}
\newlabel{fig:filtrations_left}{{10b}{17}{Ordinary persistence diagram}{figure.caption.12}{}}
\newlabel{sub@fig:filtrations_left}{{b}{17}{Ordinary persistence diagram}{figure.caption.12}{}}
\newlabel{fig:filtrations_right}{{10c}{17}{Extended persistence diagram}{figure.caption.12}{}}
\newlabel{sub@fig:filtrations_right}{{c}{17}{Extended persistence diagram}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Ordinary vs extended persistence.} (Figures adapted from \blx@tocontentsinit {0}\cite {perslay}). In \ref {fig:filtrations_top}, each node of the graph is assigned its height. Persistence intervals are shown under the sequence. In \ref {fig:filtrations_left}, ordinary persistence captures connected components and loops, but essential classes lead to infinite intervals (black and green markers) and the upward branch (blue) isn't captured. In \ref {fig:filtrations_right}, extended persistence pairs all classes at finite values by combining sublevel and superlevel filtrations with relative homology.}}{17}{figure.caption.12}\protected@file@percent }
\newlabel{fig:filtrations}{{10}{17}{\textbf {Ordinary vs extended persistence.} (Figures adapted from \cite {perslay}). In \ref {fig:filtrations_top}, each node of the graph is assigned its height. Persistence intervals are shown under the sequence. In \ref {fig:filtrations_left}, ordinary persistence captures connected components and loops, but essential classes lead to infinite intervals (black and green markers) and the upward branch (blue) isn't captured. In \ref {fig:filtrations_right}, extended persistence pairs all classes at finite values by combining sublevel and superlevel filtrations with relative homology}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Distances between diagrams.}{17}{equation.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stability theorems.}{17}{equation.8}\protected@file@percent }
\citation{persistence_landscapes}
\abx@aux@cite{0}{persistence_landscapes}
\abx@aux@segm{0}{0}{persistence_landscapes}
\citation{betti_curves}
\abx@aux@cite{0}{betti_curves}
\abx@aux@segm{0}{0}{betti_curves}
\citation{persistence_images}
\abx@aux@cite{0}{persistence_images}
\abx@aux@segm{0}{0}{persistence_images}
\citation{atol}
\abx@aux@cite{0}{atol}
\abx@aux@segm{0}{0}{atol}
\citation{adaptive_topological_feature}
\abx@aux@cite{0}{adaptive_topological_feature}
\abx@aux@segm{0}{0}{adaptive_topological_feature}
\citation{topological_signature}
\abx@aux@cite{0}{topological_signature}
\abx@aux@segm{0}{0}{topological_signature}
\citation{extended_persistence}
\abx@aux@cite{0}{extended_persistence}
\abx@aux@segm{0}{0}{extended_persistence}
\citation{perslay}
\abx@aux@cite{0}{perslay}
\abx@aux@segm{0}{0}{perslay}
\citation{pllay}
\abx@aux@cite{0}{pllay}
\abx@aux@segm{0}{0}{pllay}
\citation{persformer}
\abx@aux@cite{0}{persformer}
\abx@aux@segm{0}{0}{persformer}
\citation{multiset_transformer}
\abx@aux@cite{0}{multiset_transformer}
\abx@aux@segm{0}{0}{multiset_transformer}
\citation{xpert}
\abx@aux@cite{0}{xpert}
\abx@aux@segm{0}{0}{xpert}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Common prescribed vectorizations of a persistence diagram.} From left to right: (a) A sample persistence diagram with points representing topological features; (b) Persistence landscape capturing the prominence of features across scales; (c) Persistence image providing a smoothed, grid-based representation; \textit  {Note:} Instead of considering persistence pairs as a \textit  {(birth,death)} tuple, persistence image is fitted on $(birth, death-birth)$ pairs. (d) Betti curve showing the count of features over filtration values.}}{18}{figure.caption.13}\protected@file@percent }
\newlabel{fig:ph-vectorizations}{{11}{18}{\textbf {Common prescribed vectorizations of a persistence diagram.} From left to right: (a) A sample persistence diagram with points representing topological features; (b) Persistence landscape capturing the prominence of features across scales; (c) Persistence image providing a smoothed, grid-based representation; \textit {Note:} Instead of considering persistence pairs as a \textit {(birth,death)} tuple, persistence image is fitted on $(birth, death-birth)$ pairs. (d) Betti curve showing the count of features over filtration values}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Extended persistence.}{18}{figure.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Vectorization of Persistence Diagrams}{18}{subsubsection.4.2.2}\protected@file@percent }
\newlabel{sssec:vectorization_persistence_diagrams}{{4.2.2}{18}{Vectorization of Persistence Diagrams}{subsubsection.4.2.2}{}}
\citation{topology_aware_latent_diffusion}
\abx@aux@cite{0}{topology_aware_latent_diffusion}
\abx@aux@segm{0}{0}{topology_aware_latent_diffusion}
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{pm2ae}
\abx@aux@cite{0}{pm2ae}
\abx@aux@segm{0}{0}{pm2ae}
\citation{pcpmae}
\abx@aux@cite{0}{pcpmae}
\abx@aux@segm{0}{0}{pcpmae}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces {\bf  Classification results on ScanObjectNN.} For each backbone model, we compare performance when pretrained on ShapeNet (first row) versus ABC (second row). The third row shows the difference, with green indicating improvement and orange indicating decline when using ABC.}}{19}{table.caption.14}\protected@file@percent }
\newlabel{tb:scanobject}{{3}{19}{{\bf Classification results on ScanObjectNN.} For each backbone model, we compare performance when pretrained on ShapeNet (first row) versus ABC (second row). The third row shows the difference, with green indicating improvement and orange indicating decline when using ABC}{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Subspace Alignment Protocol}{19}{subsubsection.4.2.3}\protected@file@percent }
\newlabel{sssec:subspace_alignment_protocol}{{4.2.3}{19}{Subspace Alignment Protocol}{subsubsection.4.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Changing the Pretraining Data Distribution}{19}{subsection.4.3}\protected@file@percent }
\newlabel{ssec:changing_pretraining_data_distribution}{{4.3}{19}{Changing the Pretraining Data Distribution}{subsection.4.3}{}}
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{pm2ae}
\abx@aux@cite{0}{pm2ae}
\abx@aux@segm{0}{0}{pm2ae}
\citation{pcpmae}
\abx@aux@cite{0}{pcpmae}
\abx@aux@segm{0}{0}{pcpmae}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces {\bf  Few-shot object classification on ModelNet40 and DONUT (genus).} We conduct 10 independent experiments for each setting and report mean accuracy (\%) with standard deviation. For each backbone model, we compare performance when pretrained on ShapeNet (first row) versus ABC (second row). The third row shows the difference, with green indicating improvement and orange indicating decline when using ABC.}}{20}{table.caption.15}\protected@file@percent }
\newlabel{tb:few}{{4}{20}{{\bf Few-shot object classification on ModelNet40 and DONUT (genus).} We conduct 10 independent experiments for each setting and report mean accuracy (\%) with standard deviation. For each backbone model, we compare performance when pretrained on ShapeNet (first row) versus ABC (second row). The third row shows the difference, with green indicating improvement and orange indicating decline when using ABC}{table.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Object Classification.}{20}{table.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Few-Shot Classification.}{20}{table.caption.15}\protected@file@percent }
\citation{eulearn}
\abx@aux@cite{0}{eulearn}
\abx@aux@segm{0}{0}{eulearn}
\@writefile{toc}{\contentsline {section}{\numberline {A}Supplementary Material for DONUT}{21}{appendix.A}\protected@file@percent }
\newlabel{sec:suppl_topogen}{{A}{21}{Supplementary Material for DONUT}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}EuLearn Dataset Analysis}{21}{subsection.A.1}\protected@file@percent }
\newlabel{ssec:suppl_eulearn_analysis}{{A.1}{21}{EuLearn Dataset Analysis}{subsection.A.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {UMAP projections of Point-BERT embeddings colored by genus.} Clear clusters by genus appear for canonical orientations (left) but vanish under random rotations (right), showing that genus separation in EuLearn is tied to orientation rather than topology.}}{21}{figure.caption.16}\protected@file@percent }
\newlabel{fig:eulearn-umap-comparison}{{12}{21}{\textbf {UMAP projections of Point-BERT embeddings colored by genus.} Clear clusters by genus appear for canonical orientations (left) but vanish under random rotations (right), showing that genus separation in EuLearn is tied to orientation rather than topology}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Sampling Labels}{21}{subsection.A.2}\protected@file@percent }
\newlabel{ssec:suppl_sampling_labels}{{A.2}{21}{Sampling Labels}{subsection.A.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Classification accuracy of a $k$-nearest neighbor classifier vs. training rotation range.} Accuracy is perfect when training and testing on canonical orientations but collapses as random rotations along the z-axis are introduced, revealing orientation-dependent bias in EuLearn. \textit  {Note:} Results are cross-validated with 5 folds.}}{22}{figure.caption.17}\protected@file@percent }
\newlabel{fig:eulearn-acc-angle}{{13}{22}{\textbf {Classification accuracy of a $k$-nearest neighbor classifier vs. training rotation range.} Accuracy is perfect when training and testing on canonical orientations but collapses as random rotations along the z-axis are introduced, revealing orientation-dependent bias in EuLearn. \textit {Note:} Results are cross-validated with 5 folds}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \textbf  {Linear probing accuracy on Point-BERT embeddings of canonical vs. rotated shapes.} Probes trained on canonical orientations (\includegraphics [height=1em,valign=c]{figs/utils/unrotated.pdf}) achieve high accuracy only on canonical test data, collapsing under rotation. (\includegraphics [height=1em,valign=c]{figs/utils/rotated.pdf}). Probes trained on rotated shapes generalize poorly in both cases, confirming that EuLearn does not provide a stable signal for genus.}}{22}{figure.caption.17}\protected@file@percent }
\newlabel{tab:eulearn-overfit}{{14}{22}{\textbf {Linear probing accuracy on Point-BERT embeddings of canonical vs. rotated shapes.} Probes trained on canonical orientations (\includegraphics [height=1em,valign=c]{figs/utils/unrotated.pdf}) achieve high accuracy only on canonical test data, collapsing under rotation. (\includegraphics [height=1em,valign=c]{figs/utils/rotated.pdf}). Probes trained on rotated shapes generalize poorly in both cases, confirming that EuLearn does not provide a stable signal for genus}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Role of $k$.}{22}{subsection.A.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sampling $(\beta _0, g)$   \textbf  {Input:} $g^{\max },\tmspace  +\thickmuskip {.2777em} G^{\max },\tmspace  +\thickmuskip {.2777em} \beta _0^{\min },\tmspace  +\thickmuskip {.2777em} \beta _0^{\max },\tmspace  +\thickmuskip {.2777em} k$}}{22}{algorithm.1}\protected@file@percent }
\newlabel{alg:topogen-labels-sampling}{{1}{22}{Sampling $(\beta _0, g)$ \\ \textbf {Input:} $g^{\max },\; G^{\max },\; \beta _0^{\min },\; \beta _0^{\max },\; k$}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Choosing Components Shapes}{22}{subsection.A.3}\protected@file@percent }
\newlabel{ssec:suppl_choosing_components}{{A.3}{22}{Choosing Components Shapes}{subsection.A.3}{}}
\citation{counting}
\abx@aux@cite{0}{counting}
\abx@aux@segm{0}{0}{counting}
\newlabel{tab:topogen-hyperparams}{{\caption@xref {tab:topogen-hyperparams}{ on input line 93}}{23}{Role of $k$}{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Hyperparameter values used for the DONUT dataset for the experiments. Probing tasks boil down to classifying shapes across $\beta _0^{\max } - \beta _0^{\min } + 1 = 6$ categories for connected components and 11 categories for genera.}}{23}{table.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algorithmic Details.}{23}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Complexity.}{23}{subsection.A.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \textsc  {Enumerate-Solutions}   \textbf  {Input:} $a$, $b$, $g^{max}$   \textbf  {Output:} $S$}}{24}{algorithm.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces \textsc  {Backtrack}   \textbf  {Input:} $r_{\text  {count}}$, $r_{\text  {sum}}$, $k$, $\mathbf  {x}$, $S$   \textbf  {Output:} $S$}}{24}{algorithm.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Baselines Training Details}{24}{subsection.A.4}\protected@file@percent }
\newlabel{ssec:topogen-baseline-training}{{A.4}{24}{Baselines Training Details}{subsection.A.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparison of typical training hyperparameters for point-based models (PointNet, PointNet++, DGCNN) and topology-based models (PersFormer, xPerT, PersLay). A vertical line separates the two families of architectures.}}{24}{table.caption.19}\protected@file@percent }
\newlabel{suppl:topogen-baseline-training}{{6}{24}{Comparison of typical training hyperparameters for point-based models (PointNet, PointNet++, DGCNN) and topology-based models (PersFormer, xPerT, PersLay). A vertical line separates the two families of architectures}{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Additional Results}{25}{subsection.A.5}\protected@file@percent }
\newlabel{ssec:topogen-additional-results}{{A.5}{25}{Additional Results}{subsection.A.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Supplementary Definitions}{25}{appendix.B}\protected@file@percent }
\newlabel{sec:suppl-definitions}{{B}{25}{Supplementary Definitions}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Metrics}{25}{subsection.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Balanced accuracy}{25}{subsection.B.1}\protected@file@percent }
\newlabel{eq:balanced-accuracy}{{11}{25}{Balanced accuracy}{equation.11}{}}
\abx@aux@read@bbl@mdfivesum{D124D2922F19BD907F519270E245C3C7}
\abx@aux@defaultrefcontext{0}{persistence_images}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mantra}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{superquadrics}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{top_layer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{persistence_landscapes}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{top_signatures}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{optimizing_persistent_homology}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perslay}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shapenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pmaezero}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{extended_persistence}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{objaverse}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{objaverse_xl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{superdec}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{eulearn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{topology_activations}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{superformula}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{betti_curves}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{plato}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{topological_signature}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{topology_aware_latent_diffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{manifold}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{manifoldplus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{platonic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pllay}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xpert}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{abc}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{set-transformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{prae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{topological_autoencoders}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{adaptive_topological_feature}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pmae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pointnet++}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pointnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{persformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{atol}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{counting}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{top_reg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{multiset_transformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dogn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dgcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{persistent_homology_seg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{neural_approximation_graph_topological_features}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hfbrimae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pbert}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clay}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pm2ae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pcpmae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{thingi}{nty/global//global/global/global}
\abx@aux@defaultlabelprefix{0}{persistence_images}{}
\abx@aux@defaultlabelprefix{0}{mantra}{}
\abx@aux@defaultlabelprefix{0}{superquadrics}{}
\abx@aux@defaultlabelprefix{0}{top_layer}{}
\abx@aux@defaultlabelprefix{0}{persistence_landscapes}{}
\abx@aux@defaultlabelprefix{0}{top_signatures}{}
\abx@aux@defaultlabelprefix{0}{optimizing_persistent_homology}{}
\abx@aux@defaultlabelprefix{0}{perslay}{}
\abx@aux@defaultlabelprefix{0}{shapenet}{}
\abx@aux@defaultlabelprefix{0}{pmaezero}{}
\abx@aux@defaultlabelprefix{0}{extended_persistence}{}
\abx@aux@defaultlabelprefix{0}{objaverse}{}
\abx@aux@defaultlabelprefix{0}{objaverse_xl}{}
\abx@aux@defaultlabelprefix{0}{superdec}{}
\abx@aux@defaultlabelprefix{0}{eulearn}{}
\abx@aux@defaultlabelprefix{0}{topology_activations}{}
\abx@aux@defaultlabelprefix{0}{superformula}{}
\abx@aux@defaultlabelprefix{0}{betti_curves}{}
\abx@aux@defaultlabelprefix{0}{plato}{}
\abx@aux@defaultlabelprefix{0}{topological_signature}{}
\abx@aux@defaultlabelprefix{0}{topology_aware_latent_diffusion}{}
\abx@aux@defaultlabelprefix{0}{manifold}{}
\abx@aux@defaultlabelprefix{0}{manifoldplus}{}
\abx@aux@defaultlabelprefix{0}{platonic}{}
\abx@aux@defaultlabelprefix{0}{pllay}{}
\abx@aux@defaultlabelprefix{0}{xpert}{}
\abx@aux@defaultlabelprefix{0}{abc}{}
\abx@aux@defaultlabelprefix{0}{set-transformer}{}
\abx@aux@defaultlabelprefix{0}{prae}{}
\abx@aux@defaultlabelprefix{0}{topological_autoencoders}{}
\abx@aux@defaultlabelprefix{0}{adaptive_topological_feature}{}
\abx@aux@defaultlabelprefix{0}{pmae}{}
\abx@aux@defaultlabelprefix{0}{pointnet++}{}
\abx@aux@defaultlabelprefix{0}{pointnet}{}
\abx@aux@defaultlabelprefix{0}{persformer}{}
\abx@aux@defaultlabelprefix{0}{atol}{}
\abx@aux@defaultlabelprefix{0}{counting}{}
\abx@aux@defaultlabelprefix{0}{top_reg}{}
\abx@aux@defaultlabelprefix{0}{multiset_transformer}{}
\abx@aux@defaultlabelprefix{0}{dogn}{}
\abx@aux@defaultlabelprefix{0}{dgcnn}{}
\abx@aux@defaultlabelprefix{0}{persistent_homology_seg}{}
\abx@aux@defaultlabelprefix{0}{neural_approximation_graph_topological_features}{}
\abx@aux@defaultlabelprefix{0}{hfbrimae}{}
\abx@aux@defaultlabelprefix{0}{pbert}{}
\abx@aux@defaultlabelprefix{0}{mae}{}
\abx@aux@defaultlabelprefix{0}{clay}{}
\abx@aux@defaultlabelprefix{0}{pm2ae}{}
\abx@aux@defaultlabelprefix{0}{pcpmae}{}
\abx@aux@defaultlabelprefix{0}{thingi}{}
\gdef \@abspage@last{28}
