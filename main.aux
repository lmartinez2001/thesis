\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{biblatex}
\bibdata{main-blx,main}
\citation{biblatex-control}
\abx@aux@refcontext{nty/global//global/global/global}
\providecommand \oddpage@label [2]{}
\newlabel{sec:abstract}{{}{2}{}{Doc-Start}{}}
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{top_signatures}
\abx@aux@cite{0}{top_signatures}
\abx@aux@segm{0}{0}{top_signatures}
\citation{optimizing_persistent_homology}
\abx@aux@cite{0}{optimizing_persistent_homology}
\abx@aux@segm{0}{0}{optimizing_persistent_homology}
\citation{persistent_homology_seg}
\abx@aux@cite{0}{persistent_homology_seg}
\abx@aux@segm{0}{0}{persistent_homology_seg}
\citation{atol}
\abx@aux@cite{0}{atol}
\abx@aux@segm{0}{0}{atol}
\citation{neural_approximation_graph_topological_features}
\abx@aux@cite{0}{neural_approximation_graph_topological_features}
\abx@aux@segm{0}{0}{neural_approximation_graph_topological_features}
\citation{topological_autoencoders}
\abx@aux@cite{0}{topological_autoencoders}
\abx@aux@segm{0}{0}{topological_autoencoders}
\citation{top_layer}
\abx@aux@cite{0}{top_layer}
\abx@aux@segm{0}{0}{top_layer}
\citation{topology_activations}
\abx@aux@cite{0}{topology_activations}
\abx@aux@segm{0}{0}{topology_activations}
\citation{top_reg}
\abx@aux@cite{0}{top_reg}
\abx@aux@segm{0}{0}{top_reg}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{3}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Current State of Research}{3}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Motivations}{3}{subsection.1.2}\protected@file@percent }
\citation{objaverse}
\abx@aux@cite{0}{objaverse}
\abx@aux@segm{0}{0}{objaverse}
\citation{objaverse_xl}
\abx@aux@cite{0}{objaverse_xl}
\abx@aux@segm{0}{0}{objaverse_xl}
\citation{shapenet}
\abx@aux@cite{0}{shapenet}
\abx@aux@segm{0}{0}{shapenet}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:topogen-samples}{{\caption@xref {fig:topogen-samples}{ on input line 8}}{4}{DONUT: \underline {D}ataset \underline {O}f ma\underline {N}ifold str\underline {U}c\underline {T}ures}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Random samples from DONUT.} Despite the fact that each sample is generated from a rather small family of shapes, both the topology preserving placement and augmentations allow for a rich variety of shapes, while ensuring topological consistency.}}{4}{figure.caption.1}\protected@file@percent }
\newlabel{fig:short}{{1}{4}{\textbf {Random samples from DONUT.} Despite the fact that each sample is generated from a rather small family of shapes, both the topology preserving placement and augmentations allow for a rich variety of shapes, while ensuring topological consistency}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contributions}{4}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}DONUT: \underline  {D}ataset \underline  {O}f ma\underline  {N}ifold str\underline  {U}c\underline  {T}ures}{4}{section.2}\protected@file@percent }
\newlabel{sec:topogen}{{2}{4}{DONUT: \underline {D}ataset \underline {O}f ma\underline {N}ifold str\underline {U}c\underline {T}ures}{section.2}{}}
\citation{mantra}
\abx@aux@cite{0}{mantra}
\abx@aux@segm{0}{0}{mantra}
\citation{mantra}
\abx@aux@cite{0}{mantra}
\abx@aux@segm{0}{0}{mantra}
\citation{abc}
\abx@aux@cite{0}{abc}
\abx@aux@segm{0}{0}{abc}
\citation{thingi}
\abx@aux@cite{0}{thingi}
\abx@aux@segm{0}{0}{thingi}
\citation{eulearn}
\abx@aux@cite{0}{eulearn}
\abx@aux@segm{0}{0}{eulearn}
\citation{thingi}
\abx@aux@cite{0}{thingi}
\abx@aux@segm{0}{0}{thingi}
\citation{objaverse}
\abx@aux@cite{0}{objaverse}
\abx@aux@segm{0}{0}{objaverse}
\citation{abc}
\abx@aux@cite{0}{abc}
\abx@aux@segm{0}{0}{abc}
\citation{manifold}
\abx@aux@cite{0}{manifold}
\abx@aux@segm{0}{0}{manifold}
\citation{manifoldplus}
\abx@aux@cite{0}{manifoldplus}
\abx@aux@segm{0}{0}{manifoldplus}
\citation{dogn}
\abx@aux@cite{0}{dogn}
\abx@aux@segm{0}{0}{dogn}
\citation{clay}
\abx@aux@cite{0}{clay}
\abx@aux@segm{0}{0}{clay}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Existing datasets}{5}{subsection.2.1}\protected@file@percent }
\newlabel{ssec:existing_datasets}{{2.1}{5}{Existing datasets}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Combinatorial benchmarks.}{5}{subsection.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Overview of existing datasets and their capabilities.} We summarize here the main characteristics of existing datasets with topological annotations. Besides EuLearn, all existing datasets with topological annotations come with downsides, discussed in Section~\ref {ssec:existing_datasets}. However, since EuLearn seems to be the most promising dataset, we carried out an extensive analysis to (1) highlight limitations that make it unreliable for further experiments and (2) motivate the use of DONUT (see Appendix~\ref {ssec:suppl_eulearn_analysis}). \textit  {Note:} The number of samples for MANTRA only takes into account 2-manifolds.}}{5}{table.caption.2}\protected@file@percent }
\newlabel{tab:datasets}{{1}{5}{\textbf {Overview of existing datasets and their capabilities.} We summarize here the main characteristics of existing datasets with topological annotations. Besides EuLearn, all existing datasets with topological annotations come with downsides, discussed in Section~\ref {ssec:existing_datasets}. However, since EuLearn seems to be the most promising dataset, we carried out an extensive analysis to (1) highlight limitations that make it unreliable for further experiments and (2) motivate the use of DONUT (see Appendix~\ref {ssec:suppl_eulearn_analysis}). \textit {Note:} The number of samples for MANTRA only takes into account 2-manifolds}{table.caption.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Noisy topology.}{5}{table.caption.2}\protected@file@percent }
\newlabel{eq:euler}{{1}{5}{Noisy topology}{equation.1}{}}
\citation{eulearn}
\abx@aux@cite{0}{eulearn}
\abx@aux@segm{0}{0}{eulearn}
\newlabel{fig:thingi-genus}{{2a}{6}{\textbf {Genus distribution of Thingi10K.} Both axes are in log scale. The genus, estimated from the Euler characteristic provided as metadata, could not be computed for 2,651 samples, which are excluded. The histogram shows that most meshes (over 3,000 of 7,344) have genus 0 or 1, highlighting the scarcity of higher-genus shapes}{figure.caption.3}{}}
\newlabel{sub@fig:thingi-genus}{{a}{6}{\textbf {Genus distribution of Thingi10K.} Both axes are in log scale. The genus, estimated from the Euler characteristic provided as metadata, could not be computed for 2,651 samples, which are excluded. The histogram shows that most meshes (over 3,000 of 7,344) have genus 0 or 1, highlighting the scarcity of higher-genus shapes}{figure.caption.3}{}}
\newlabel{fig:thingi-comps}{{2b}{6}{\textbf {Raw connected components.} The left figure shows a mesh made of 8 connected components, highlighted with different colors. However, once converted into a point cloud (right figure), we lose track of this information. Therefore, labeling this sample as having 8 connected components would be misleading for a model}{figure.caption.3}{}}
\newlabel{sub@fig:thingi-comps}{{b}{6}{\textbf {Raw connected components.} The left figure shows a mesh made of 8 connected components, highlighted with different colors. However, once converted into a point cloud (right figure), we lose track of this information. Therefore, labeling this sample as having 8 connected components would be misleading for a model}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:thingi-overview}{{2}{6}{\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Synthetic datasets with limited diversity.}{6}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Samples from the EuLearn dataset across different genera.} As the genus increases, shapes become geometrically more complex. This trend highlights a confounding factor in the dataset: geometric complexity grows together with genus.}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:eulearn-samples}{{3}{6}{\textbf {Samples from the EuLearn dataset across different genera.} As the genus increases, shapes become geometrically more complex. This trend highlights a confounding factor in the dataset: geometric complexity grows together with genus}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Method}{6}{subsection.2.2}\protected@file@percent }
\newlabel{ssec:topogen-method}{{2.2}{6}{Method}{subsection.2.2}{}}
\citation{superquadrics}
\abx@aux@cite{0}{superquadrics}
\abx@aux@segm{0}{0}{superquadrics}
\citation{superdec}
\abx@aux@cite{0}{superdec}
\abx@aux@segm{0}{0}{superdec}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Generation Process Overview.} (1) For each sample, we pick a target genus and number of connected components. (Section~\ref {sssec:labels-distribution}) (2) We then distribute these values across a set of template shapes (Section~\ref {sssec:sample-level-properties}). (3) Each mesh is instantiated by sampling parameters from its parametric expression, and individually warped to bring more geometric diversity. (4) Finally, we apply a series of topology-preserving augmentations to increase geometric diversity while maintaining the original topological labels, and compose the final sample by merging all meshes. (Section~\ref {sssec:mesh-generation})}}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig:topogen-overview}{{4}{7}{\textbf {Generation Process Overview.} (1) For each sample, we pick a target genus and number of connected components. (Section~\ref {sssec:labels-distribution}) (2) We then distribute these values across a set of template shapes (Section~\ref {sssec:sample-level-properties}). (3) Each mesh is instantiated by sampling parameters from its parametric expression, and individually warped to bring more geometric diversity. (4) Finally, we apply a series of topology-preserving augmentations to increase geometric diversity while maintaining the original topological labels, and compose the final sample by merging all meshes. (Section~\ref {sssec:mesh-generation})}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {UMAP embeddings of features learned by 3D encoders.}}}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:topogen-umaps-overview}{{5}{7}{\textbf {UMAP embeddings of features learned by 3D encoders.}}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Topological Label Sampling}{7}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{sssec:labels-distribution}{{2.2.1}{7}{Topological Label Sampling}{subsubsection.2.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Shape Instantiation}{7}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{sssec:sample-level-properties}{{2.2.2}{7}{Shape Instantiation}{subsubsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Meshes Generation}{8}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{sssec:mesh-generation}{{2.2.3}{8}{Meshes Generation}{subsubsection.2.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Cones.}{8}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Superquadrics.}{8}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$K$-tori.}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig:toroids-overview}{{6a}{9}{Supertoroids}{figure.caption.7}{}}
\newlabel{sub@fig:toroids-overview}{{a}{9}{Supertoroids}{figure.caption.7}{}}
\newlabel{fig:ellipsoids-overview}{{6b}{9}{Superellipsoids}{figure.caption.7}{}}
\newlabel{sub@fig:ellipsoids-overview}{{b}{9}{Superellipsoids}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Overview of different shapes obtained for fixed values of $a_i, i \in \{1, 2, 3\}$ and increasing values of $\epsilon _1$ (left to right) and $\epsilon _2$ (bottom to top). (a) Different supertoroids. As mentioned in TO ADD, using these shapes for $k$-tori ($k \geq 2$) is challenging because they may not preserve the genus, for instance if some parts are too thin or sharp. (b) Different superellipsoids.}}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:overview}{{6}{9}{Overview of different shapes obtained for fixed values of $a_i, i \in \{1, 2, 3\}$ and increasing values of $\epsilon _1$ (left to right) and $\epsilon _2$ (bottom to top). (a) Different supertoroids. As mentioned in TO ADD, using these shapes for $k$-tori ($k \geq 2$) is challenging because they may not preserve the genus, for instance if some parts are too thin or sharp. (b) Different superellipsoids}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Topological Consistency and Diversity}{9}{subsubsection.2.2.4}\protected@file@percent }
\newlabel{sssec:top-consistency}{{2.2.4}{9}{Topological Consistency and Diversity}{subsubsection.2.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Topological Consistency.}{9}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variability.}{9}{figure.caption.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Component placement procedure.} (1) A new component is randomly placed within the sample. (2) If it overlaps with existing components, we identify the point on the new mesh that is most deeply inside the other component and push the new component along the opposite direction of the surface normal at that point. This step is repeated up to five times. If overlaps remain, the component is discarded and a new one is generated and randomly placed. This procedure ensures that components do not overlap while remaining close enough to preserve topological consistency and challenge models to correctly identify the number of connected components.}}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:topogen-placement}{{7}{10}{\textbf {Component placement procedure.} (1) A new component is randomly placed within the sample. (2) If it overlaps with existing components, we identify the point on the new mesh that is most deeply inside the other component and push the new component along the opposite direction of the surface normal at that point. This step is repeated up to five times. If overlaps remain, the component is discarded and a new one is generated and randomly placed. This procedure ensures that components do not overlap while remaining close enough to preserve topological consistency and challenge models to correctly identify the number of connected components}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Analysis}{10}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}General Properties}{10}{subsubsection.2.3.1}\protected@file@percent }
\newlabel{sssec:topogen-general-properties}{{2.3.1}{10}{General Properties}{subsubsection.2.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Dataset statistics.}{10}{figure.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Distributions and scalability.}{10}{figure.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Baselines and Transferability}{10}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{sssec:topogen-transferability}{{2.3.2}{10}{Baselines and Transferability}{subsubsection.2.3.2}{}}
\citation{pointnet}
\abx@aux@cite{0}{pointnet}
\abx@aux@segm{0}{0}{pointnet}
\citation{pointnet++}
\abx@aux@cite{0}{pointnet++}
\abx@aux@segm{0}{0}{pointnet++}
\citation{dgcnn}
\abx@aux@cite{0}{dgcnn}
\abx@aux@segm{0}{0}{dgcnn}
\citation{pointnet}
\abx@aux@cite{0}{pointnet}
\abx@aux@segm{0}{0}{pointnet}
\citation{pointnet++}
\abx@aux@cite{0}{pointnet++}
\abx@aux@segm{0}{0}{pointnet++}
\citation{dgcnn}
\abx@aux@cite{0}{dgcnn}
\abx@aux@segm{0}{0}{dgcnn}
\newlabel{fig:ktori-overview}{{8a}{11}{\textbf {Degrees of freedom allowed on $k$-tori.} Prior to the sequence of transformations applied to each individual component (e.g. rotation, twisting) we allow some variability during the generation. The x-axis represents the ratio \textit {major radius / minor radius}. The y-axis shows how the template shape is modified as we increase the value of $k$ (i.e. the genus). \textit {Note:} In the actual dataset, $1$-tori are generated with the parametric representation of supertoroids, since a regular torus is a particular case of supertoroid. However, for $k \geq 2$, using a composition of $1$-tori is more reliable. Some parameter sets can lead to undesirable holes in the final shape}{figure.caption.9}{}}
\newlabel{sub@fig:ktori-overview}{{a}{11}{\textbf {Degrees of freedom allowed on $k$-tori.} Prior to the sequence of transformations applied to each individual component (e.g. rotation, twisting) we allow some variability during the generation. The x-axis represents the ratio \textit {major radius / minor radius}. The y-axis shows how the template shape is modified as we increase the value of $k$ (i.e. the genus). \textit {Note:} In the actual dataset, $1$-tori are generated with the parametric representation of supertoroids, since a regular torus is a particular case of supertoroid. However, for $k \geq 2$, using a composition of $1$-tori is more reliable. Some parameter sets can lead to undesirable holes in the final shape}{figure.caption.9}{}}
\newlabel{fig:twisted-comparison}{{8b}{11}{\textbf {Effect of twisting.} Original template shapes (left) are twisted along the purple axis (right). Twisting deformations introduce non-rigid variability while preserving the genus and connectivity of each component. Here the scalar function defined along the axis is affine between $-\pi /6$ and $\pi /3$. We apply this augmentation at the component and sample level}{figure.caption.9}{}}
\newlabel{sub@fig:twisted-comparison}{{b}{11}{\textbf {Effect of twisting.} Original template shapes (left) are twisted along the purple axis (right). Twisting deformations introduce non-rigid variability while preserving the genus and connectivity of each component. Here the scalar function defined along the axis is affine between $-\pi /6$ and $\pi /3$. We apply this augmentation at the component and sample level}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Results.}{11}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Saliency Analysis}{11}{subsubsection.2.3.3}\protected@file@percent }
\newlabel{sssec:topogen-saliency}{{2.3.3}{11}{Saliency Analysis}{subsubsection.2.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition.}{11}{figure.caption.12}\protected@file@percent }
\citation{smoothgrad}
\abx@aux@cite{0}{smoothgrad}
\abx@aux@segm{0}{0}{smoothgrad}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Label distributions for genus and connected components.} The marginal distributions of genus and connected components (left and middle histograms) are approximately uniform, ensuring balanced supervision across topological classes. The right plot shows the joint distribution. The absence of samples with one connected component and genus greater than 5 stems from the constraints of our generation process, which limits individual components to a maximum genus of 5. The only way to achieve a genus greater than 5 is by combining multiple components.}}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:topogen-properties}{{9}{12}{\textbf {Label distributions for genus and connected components.} The marginal distributions of genus and connected components (left and middle histograms) are approximately uniform, ensuring balanced supervision across topological classes. The right plot shows the joint distribution. The absence of samples with one connected component and genus greater than 5 stems from the constraints of our generation process, which limits individual components to a maximum genus of 5. The only way to achieve a genus greater than 5 is by combining multiple components}{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Performance of baseline models trained on DONUT and evaluated both in-distribution and on ABC.} Values are reported as \textit  {DONUT/ABC}. We report mean squared error (MSE), balanced accuracy (Acc.), and balanced F1-score (F1). MSE is reported to capture how far off predictions are from the ground-truth on average, which is particularly relevant given the natural hierarchy of the labels (genus and connected components): misclassifying by a larger margin is more severe than a closer miss. Training setup is detailed in Table~\ref {suppl:topogen-baseline-training}}}{12}{table.caption.11}\protected@file@percent }
\newlabel{tab:topogen-results}{{2}{12}{\textbf {Performance of baseline models trained on DONUT and evaluated both in-distribution and on ABC.} Values are reported as \textit {DONUT/ABC}. We report mean squared error (MSE), balanced accuracy (Acc.), and balanced F1-score (F1). MSE is reported to capture how far off predictions are from the ground-truth on average, which is particularly relevant given the natural hierarchy of the labels (genus and connected components): misclassifying by a larger margin is more severe than a closer miss. Training setup is detailed in Table~\ref {suppl:topogen-baseline-training}}{table.caption.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation on point clouds.}{12}{equation.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical computation.}{12}{equation.6}\protected@file@percent }
\citation{superformula}
\abx@aux@cite{0}{superformula}
\abx@aux@segm{0}{0}{superformula}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Saliency maps of toy shapes with varying topology.} We show saliency for DGCNN as it's the best performing model on DONUT (Table~\ref {tab:topogen-results}). Top row shows saliency maps on genus prediction for samples with a single connected component and increasing genus. We also specify the genus predicted by the model. Bottom row highlights how saliency maps change between genus (left) and connected component (right) prediction for the same sample with two connected components.}}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig:topogen-saliencies}{{10}{13}{\textbf {Saliency maps of toy shapes with varying topology.} We show saliency for DGCNN as it's the best performing model on DONUT (Table~\ref {tab:topogen-results}). Top row shows saliency maps on genus prediction for samples with a single connected component and increasing genus. We also specify the genus predicted by the model. Bottom row highlights how saliency maps change between genus (left) and connected component (right) prediction for the same sample with two connected components}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Results.}{13}{equation.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Limitations and Further Improvements}{13}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Topological Variety.}{13}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometric Complexity.}{13}{subsection.2.4}\protected@file@percent }
\citation{set-transformer}
\abx@aux@cite{0}{set-transformer}
\abx@aux@segm{0}{0}{set-transformer}
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{pm2ae}
\abx@aux@cite{0}{pm2ae}
\abx@aux@segm{0}{0}{pm2ae}
\citation{pcpmae}
\abx@aux@cite{0}{pcpmae}
\abx@aux@segm{0}{0}{pcpmae}
\citation{pointnet}
\abx@aux@cite{0}{pointnet}
\abx@aux@segm{0}{0}{pointnet}
\citation{pointnet++}
\abx@aux@cite{0}{pointnet++}
\abx@aux@segm{0}{0}{pointnet++}
\citation{dgcnn}
\abx@aux@cite{0}{dgcnn}
\abx@aux@segm{0}{0}{dgcnn}
\citation{pointcnn}
\abx@aux@cite{0}{pointcnn}
\abx@aux@segm{0}{0}{pointcnn}
\citation{pointconv}
\abx@aux@cite{0}{pointconv}
\abx@aux@segm{0}{0}{pointconv}
\citation{kpconv}
\abx@aux@cite{0}{kpconv}
\abx@aux@segm{0}{0}{kpconv}
\citation{pvcnn}
\abx@aux@cite{0}{pvcnn}
\abx@aux@segm{0}{0}{pvcnn}
\@writefile{toc}{\contentsline {section}{\numberline {3}Used Models}{14}{section.3}\protected@file@percent }
\newlabel{sec:used_models}{{3}{14}{Used Models}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Timeline of models used as 3D point-cloud encoders}}}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig:timeline}{{11}{14}{\textbf {Timeline of models used as 3D point-cloud encoders}}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Baselines Models}{14}{subsection.3.1}\protected@file@percent }
\newlabel{ssec:baselines_models}{{3.1}{14}{Baselines Models}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}PointNet}{14}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}PointNet++}{14}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}DGCNN (Dynamic Graph CNN)}{15}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Transformer-based Models}{15}{subsection.3.2}\protected@file@percent }
\newlabel{ssec:transformer_based_models}{{3.2}{15}{Transformer-based Models}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Transformer Overview for 3D point-clouds.} A point cloud is first partitioned into local patches using farthest point sampling (FPS) and K nearest neighbors (KNN). Patches are then embedded and augmented with positional encodings. The resulting sequence of patch embeddings is processed by a Transformer encoder to produce a global shape representation.}}{15}{figure.caption.14}\protected@file@percent }
\newlabel{fig:transformer-overview}{{12}{15}{\textbf {Transformer Overview for 3D point-clouds.} A point cloud is first partitioned into local patches using farthest point sampling (FPS) and K nearest neighbors (KNN). Patches are then embedded and augmented with positional encodings. The resulting sequence of patch embeddings is processed by a Transformer encoder to produce a global shape representation}{figure.caption.14}{}}
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Point-BERT: Discrete Tokenization and Masked Modeling}{16}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{sssec:pointbert}{{3.2.1}{16}{Point-BERT: Discrete Tokenization and Masked Modeling}{subsubsection.3.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Point-MAE: Continuous Tokens and Masked Autoencoding}{16}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{sssec:pointmae}{{3.2.2}{16}{Point-MAE: Continuous Tokens and Masked Autoencoding}{subsubsection.3.2.2}{}}
\citation{pm2ae}
\abx@aux@cite{0}{pm2ae}
\abx@aux@segm{0}{0}{pm2ae}
\citation{pcpmae}
\abx@aux@cite{0}{pcpmae}
\abx@aux@segm{0}{0}{pcpmae}
\citation{hfbrimae}
\abx@aux@cite{0}{hfbrimae}
\abx@aux@segm{0}{0}{hfbrimae}
\citation{prae}
\abx@aux@cite{0}{prae}
\abx@aux@segm{0}{0}{prae}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Hierarchical Extensions: Multi-Scale Representations}{17}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{sssec:hierarchical_extensions}{{3.2.3}{17}{Hierarchical Extensions: Multi-Scale Representations}{subsubsection.3.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Further Improvements}{17}{subsubsection.3.2.4}\protected@file@percent }
\newlabel{sssec:further_improvements}{{3.2.4}{17}{Further Improvements}{subsubsection.3.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Comparative View}{17}{subsubsection.3.2.5}\protected@file@percent }
\newlabel{sssec:comparative_view}{{3.2.5}{17}{Comparative View}{subsubsection.3.2.5}{}}
\citation{ulip}
\abx@aux@cite{0}{ulip}
\abx@aux@segm{0}{0}{ulip}
\citation{ulip2}
\abx@aux@cite{0}{ulip2}
\abx@aux@segm{0}{0}{ulip2}
\citation{pointllm}
\abx@aux@cite{0}{pointllm}
\abx@aux@segm{0}{0}{pointllm}
\citation{shapellm}
\abx@aux@cite{0}{shapellm}
\abx@aux@segm{0}{0}{shapellm}
\citation{uni3d}
\abx@aux@cite{0}{uni3d}
\abx@aux@segm{0}{0}{uni3d}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Multimodal approaches}{18}{subsection.3.3}\protected@file@percent }
\newlabel{ssec:multimodal}{{3.3}{18}{Multimodal approaches}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Why we focus on unimodal encoders.}{18}{equation.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{18}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{18}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Probing}{18}{subsection.4.1}\protected@file@percent }
\newlabel{ssec:model_probing}{{4.1}{18}{Model Probing}{subsection.4.1}{}}
\citation{intermediate_layers}
\abx@aux@cite{0}{intermediate_layers}
\abx@aux@segm{0}{0}{intermediate_layers}
\citation{structural_probe}
\abx@aux@cite{0}{structural_probe}
\abx@aux@segm{0}{0}{structural_probe}
\citation{vit_probing}
\abx@aux@cite{0}{vit_probing}
\abx@aux@segm{0}{0}{vit_probing}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Linear probing accuracy across transformer layers for Point-BERT (left column) and Point-MAE (right column).} We report probing accuracy for connected components (top row) and genus (bottom row). For each setting, accuracy values are averaged over the validation sets from a 5-fold cross-validation procedure, with error bars representing the corresponding standard deviations. For Point-BERT, probing is applied to the CLS token, which aggregates global information throughout the network. For Point-MAE, which does not include a CLS token during pretraining, we instead use the max-pooled patch embeddings from each layer.}}{19}{figure.caption.15}\protected@file@percent }
\newlabel{fig:probing-results}{{13}{19}{\textbf {Linear probing accuracy across transformer layers for Point-BERT (left column) and Point-MAE (right column).} We report probing accuracy for connected components (top row) and genus (bottom row). For each setting, accuracy values are averaged over the validation sets from a 5-fold cross-validation procedure, with error bars representing the corresponding standard deviations. For Point-BERT, probing is applied to the CLS token, which aggregates global information throughout the network. For Point-MAE, which does not include a CLS token during pretraining, we instead use the max-pooled patch embeddings from each layer}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation.}{19}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental setup.}{19}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results.}{19}{figure.caption.15}\protected@file@percent }
\citation{plato}
\abx@aux@cite{0}{plato}
\abx@aux@segm{0}{0}{plato}
\citation{platonic}
\abx@aux@cite{0}{platonic}
\abx@aux@segm{0}{0}{platonic}
\citation{platonic}
\abx@aux@cite{0}{platonic}
\abx@aux@segm{0}{0}{platonic}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Subspace Alignment with Persistent Homology}{20}{subsection.4.2}\protected@file@percent }
\newlabel{ssec:ph}{{4.2}{20}{Subspace Alignment with Persistent Homology}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \textbf  {Alignment procedure overview.} We extract features from a 3D point cloud with the model we seek to evaluate. We also vectorize its corresponding persistence diagram (Section~\ref {sssec:vectorization_persistence_diagrams}). We then compute the CKA similarity (Section~\ref {sssec:subspace_alignment_protocol}) between the two sets of features to quantify how well the learned representation aligns with topological signatures.}}{20}{figure.caption.16}\protected@file@percent }
\newlabel{fig:alignment}{{14}{20}{\textbf {Alignment procedure overview.} We extract features from a 3D point cloud with the model we seek to evaluate. We also vectorize its corresponding persistence diagram (Section~\ref {sssec:vectorization_persistence_diagrams}). We then compute the CKA similarity (Section~\ref {sssec:subspace_alignment_protocol}) between the two sets of features to quantify how well the learned representation aligns with topological signatures}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Persistence Diagrams}{20}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{sssec:persistence_diagrams}{{4.2.1}{20}{Persistence Diagrams}{subsubsection.4.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Filtrations.}{20}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Homology and persistent maps.}{20}{subsubsection.4.2.1}\protected@file@percent }
\citation{perslay}
\abx@aux@cite{0}{perslay}
\abx@aux@segm{0}{0}{perslay}
\citation{perslay}
\abx@aux@cite{0}{perslay}
\abx@aux@segm{0}{0}{perslay}
\@writefile{toc}{\contentsline {paragraph}{Persistence modules, barcodes and diagrams.}{21}{equation.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why a persistence diagram is not a Euclidean vector object.}{21}{equation.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Distances between diagrams.}{21}{equation.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stability theorems.}{21}{equation.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extended persistence.}{21}{figure.caption.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Vectorization of Persistence Diagrams}{21}{subsubsection.4.2.2}\protected@file@percent }
\newlabel{sssec:vectorization_persistence_diagrams}{{4.2.2}{21}{Vectorization of Persistence Diagrams}{subsubsection.4.2.2}{}}
\citation{persistence_landscapes}
\abx@aux@cite{0}{persistence_landscapes}
\abx@aux@segm{0}{0}{persistence_landscapes}
\citation{betti_curves}
\abx@aux@cite{0}{betti_curves}
\abx@aux@segm{0}{0}{betti_curves}
\citation{persistence_images}
\abx@aux@cite{0}{persistence_images}
\abx@aux@segm{0}{0}{persistence_images}
\citation{atol}
\abx@aux@cite{0}{atol}
\abx@aux@segm{0}{0}{atol}
\citation{adaptive_topological_feature}
\abx@aux@cite{0}{adaptive_topological_feature}
\abx@aux@segm{0}{0}{adaptive_topological_feature}
\citation{topological_signature}
\abx@aux@cite{0}{topological_signature}
\abx@aux@segm{0}{0}{topological_signature}
\citation{extended_persistence}
\abx@aux@cite{0}{extended_persistence}
\abx@aux@segm{0}{0}{extended_persistence}
\citation{perslay}
\abx@aux@cite{0}{perslay}
\abx@aux@segm{0}{0}{perslay}
\citation{pllay}
\abx@aux@cite{0}{pllay}
\abx@aux@segm{0}{0}{pllay}
\citation{persformer}
\abx@aux@cite{0}{persformer}
\abx@aux@segm{0}{0}{persformer}
\citation{multiset_transformer}
\abx@aux@cite{0}{multiset_transformer}
\abx@aux@segm{0}{0}{multiset_transformer}
\citation{xpert}
\abx@aux@cite{0}{xpert}
\abx@aux@segm{0}{0}{xpert}
\newlabel{fig:filtrations_top}{{15a}{22}{Sub- (resp. super-) level set filtration}{figure.caption.17}{}}
\newlabel{sub@fig:filtrations_top}{{a}{22}{Sub- (resp. super-) level set filtration}{figure.caption.17}{}}
\newlabel{fig:filtrations_left}{{15b}{22}{Ordinary persistence diagram}{figure.caption.17}{}}
\newlabel{sub@fig:filtrations_left}{{b}{22}{Ordinary persistence diagram}{figure.caption.17}{}}
\newlabel{fig:filtrations_right}{{15c}{22}{Extended persistence diagram}{figure.caption.17}{}}
\newlabel{sub@fig:filtrations_right}{{c}{22}{Extended persistence diagram}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \textbf  {Ordinary vs extended persistence.} (Figures adapted from \blx@tocontentsinit {0}\cite {perslay}). In \ref {fig:filtrations_top}, each node of the graph is assigned its height. Persistence intervals are shown under the sequence. In \ref {fig:filtrations_left}, ordinary persistence captures connected components and loops, but essential classes lead to infinite intervals (black and green markers) and the upward branch (blue) isn't captured. In \ref {fig:filtrations_right}, extended persistence pairs all classes at finite values by combining sublevel and superlevel filtrations with relative homology. Finally, $\text  {Ext}_0^+$, $\text  {Ext}_1^-$, $\text  {Ord}_0$ and $\text  {Ord}_1$ denote the different types of pairs in extended persistence.}}{22}{figure.caption.17}\protected@file@percent }
\newlabel{fig:filtrations}{{15}{22}{\textbf {Ordinary vs extended persistence.} (Figures adapted from \cite {perslay}). In \ref {fig:filtrations_top}, each node of the graph is assigned its height. Persistence intervals are shown under the sequence. In \ref {fig:filtrations_left}, ordinary persistence captures connected components and loops, but essential classes lead to infinite intervals (black and green markers) and the upward branch (blue) isn't captured. In \ref {fig:filtrations_right}, extended persistence pairs all classes at finite values by combining sublevel and superlevel filtrations with relative homology. Finally, $\text {Ext}_0^+$, $\text {Ext}_1^-$, $\text {Ord}_0$ and $\text {Ord}_1$ denote the different types of pairs in extended persistence}{figure.caption.17}{}}
\citation{topology_aware_latent_diffusion}
\abx@aux@cite{0}{topology_aware_latent_diffusion}
\abx@aux@segm{0}{0}{topology_aware_latent_diffusion}
\citation{cka}
\abx@aux@cite{0}{cka}
\abx@aux@segm{0}{0}{cka}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces \textbf  {Common prescribed vectorizations of a persistence diagram.} From left to right: (a) A sample persistence diagram with points representing topological features; (b) Persistence landscape capturing the prominence of features across scales; (c) Persistence image providing a smoothed, grid-based representation; \textit  {Note:} Instead of considering persistence pairs as a \textit  {(birth,death)} tuple, persistence image is fitted on $(birth, death-birth)$ pairs. (d) Betti curve showing the count of features over filtration values.}}{23}{figure.caption.18}\protected@file@percent }
\newlabel{fig:ph-vectorizations}{{16}{23}{\textbf {Common prescribed vectorizations of a persistence diagram.} From left to right: (a) A sample persistence diagram with points representing topological features; (b) Persistence landscape capturing the prominence of features across scales; (c) Persistence image providing a smoothed, grid-based representation; \textit {Note:} Instead of considering persistence pairs as a \textit {(birth,death)} tuple, persistence image is fitted on $(birth, death-birth)$ pairs. (d) Betti curve showing the count of features over filtration values}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Subspace Alignment Protocol}{23}{subsubsection.4.2.3}\protected@file@percent }
\newlabel{sssec:subspace_alignment_protocol}{{4.2.3}{23}{Subspace Alignment Protocol}{subsubsection.4.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Background}{23}{subsubsection.4.2.3}\protected@file@percent }
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{pm2ae}
\abx@aux@cite{0}{pm2ae}
\abx@aux@segm{0}{0}{pm2ae}
\citation{pcpmae}
\abx@aux@cite{0}{pcpmae}
\abx@aux@segm{0}{0}{pcpmae}
\@writefile{toc}{\contentsline {paragraph}{Experimental setting.}{24}{equation.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results}{24}{equation.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Changing the Pretraining Data Distribution}{24}{subsection.4.3}\protected@file@percent }
\newlabel{ssec:changing_pretraining_data_distribution}{{4.3}{24}{Changing the Pretraining Data Distribution}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces \textbf  {Point-cloud reconstruction for different encoders.} Each pretrained model is fed with a point cloud with 2048 points. We reconstruct it with the decoder used during pretraining with a masking ratio of zero. To ensure topological structures of the input are captured, we patchify the input point cloud into overlapping local regions before encoding, and decode to a reconstructed point cloud of 4096 points. Reconstructed point-clouds don't exhibit the 3 characteristic holes of the input shape.}}{24}{figure.caption.19}\protected@file@percent }
\newlabel{fig:reconstructions}{{17}{24}{\textbf {Point-cloud reconstruction for different encoders.} Each pretrained model is fed with a point cloud with 2048 points. We reconstruct it with the decoder used during pretraining with a masking ratio of zero. To ensure topological structures of the input are captured, we patchify the input point cloud into overlapping local regions before encoding, and decode to a reconstructed point cloud of 4096 points. Reconstructed point-clouds don't exhibit the 3 characteristic holes of the input shape}{figure.caption.19}{}}
\citation{pbert}
\abx@aux@cite{0}{pbert}
\abx@aux@segm{0}{0}{pbert}
\citation{pmae}
\abx@aux@cite{0}{pmae}
\abx@aux@segm{0}{0}{pmae}
\citation{pm2ae}
\abx@aux@cite{0}{pm2ae}
\abx@aux@segm{0}{0}{pm2ae}
\citation{pcpmae}
\abx@aux@cite{0}{pcpmae}
\abx@aux@segm{0}{0}{pcpmae}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces {\bf  Classification results on ScanObjectNN.} For each backbone model, we compare performance when pretrained on ShapeNet (first row) versus ABC (second row). The third row shows the difference, with green indicating improvement and orange indicating decline when using ABC.}}{25}{table.caption.20}\protected@file@percent }
\newlabel{tb:scanobject}{{3}{25}{{\bf Classification results on ScanObjectNN.} For each backbone model, we compare performance when pretrained on ShapeNet (first row) versus ABC (second row). The third row shows the difference, with green indicating improvement and orange indicating decline when using ABC}{table.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{Object Classification.}{25}{table.caption.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Few-Shot Classification.}{25}{table.caption.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{25}{section.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces {\bf  Few-shot object classification on ModelNet40 and DONUT (genus).} We conduct 10 independent experiments for each setting and report mean accuracy (\%) with standard deviation. For each backbone model, we compare performance when pretrained on ShapeNet (first row) versus ABC (second row). The third row shows the difference, with green indicating improvement and orange indicating decline when using ABC.}}{26}{table.caption.21}\protected@file@percent }
\newlabel{tb:few}{{4}{26}{{\bf Few-shot object classification on ModelNet40 and DONUT (genus).} We conduct 10 independent experiments for each setting and report mean accuracy (\%) with standard deviation. For each backbone model, we compare performance when pretrained on ShapeNet (first row) versus ABC (second row). The third row shows the difference, with green indicating improvement and orange indicating decline when using ABC}{table.caption.21}{}}
\citation{eulearn}
\abx@aux@cite{0}{eulearn}
\abx@aux@segm{0}{0}{eulearn}
\@writefile{toc}{\contentsline {section}{\numberline {A}Supplementary Material for DONUT}{27}{appendix.A}\protected@file@percent }
\newlabel{sec:suppl_topogen}{{A}{27}{Supplementary Material for DONUT}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}EuLearn Dataset Analysis}{27}{subsection.A.1}\protected@file@percent }
\newlabel{ssec:suppl_eulearn_analysis}{{A.1}{27}{EuLearn Dataset Analysis}{subsection.A.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces \textbf  {UMAP projections of Point-BERT embeddings colored by genus.} Clear clusters by genus appear for canonical orientations (left) but vanish under random rotations (right), showing that genus separation in EuLearn is tied to orientation rather than topology.}}{27}{figure.caption.22}\protected@file@percent }
\newlabel{fig:eulearn-umap-comparison}{{18}{27}{\textbf {UMAP projections of Point-BERT embeddings colored by genus.} Clear clusters by genus appear for canonical orientations (left) but vanish under random rotations (right), showing that genus separation in EuLearn is tied to orientation rather than topology}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Sampling Labels}{27}{subsection.A.2}\protected@file@percent }
\newlabel{ssec:suppl_sampling_labels}{{A.2}{27}{Sampling Labels}{subsection.A.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces \textbf  {Classification accuracy of a $k$-nearest neighbor classifier vs. training rotation range.} Accuracy is perfect when training and testing on canonical orientations but collapses as random rotations along the z-axis are introduced, revealing orientation-dependent bias in EuLearn. \textit  {Note:} Results are cross-validated with 5 folds.}}{28}{figure.caption.23}\protected@file@percent }
\newlabel{fig:eulearn-acc-angle}{{19}{28}{\textbf {Classification accuracy of a $k$-nearest neighbor classifier vs. training rotation range.} Accuracy is perfect when training and testing on canonical orientations but collapses as random rotations along the z-axis are introduced, revealing orientation-dependent bias in EuLearn. \textit {Note:} Results are cross-validated with 5 folds}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces \textbf  {Linear probing accuracy on Point-BERT embeddings of canonical vs. rotated shapes.} Probes trained on canonical orientations (\includegraphics [height=1em,valign=c]{figs/utils/unrotated.pdf}) achieve high accuracy only on canonical test data, collapsing under rotation. (\includegraphics [height=1em,valign=c]{figs/utils/rotated.pdf}). Probes trained on rotated shapes generalize poorly in both cases, confirming that EuLearn does not provide a stable signal for genus.}}{28}{figure.caption.23}\protected@file@percent }
\newlabel{tab:eulearn-overfit}{{20}{28}{\textbf {Linear probing accuracy on Point-BERT embeddings of canonical vs. rotated shapes.} Probes trained on canonical orientations (\includegraphics [height=1em,valign=c]{figs/utils/unrotated.pdf}) achieve high accuracy only on canonical test data, collapsing under rotation. (\includegraphics [height=1em,valign=c]{figs/utils/rotated.pdf}). Probes trained on rotated shapes generalize poorly in both cases, confirming that EuLearn does not provide a stable signal for genus}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Role of $k$.}{28}{subsection.A.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sampling $(\beta _0, g)$   \textbf  {Input:} $g^{\max },\tmspace  +\thickmuskip {.2777em} G^{\max },\tmspace  +\thickmuskip {.2777em} \beta _0^{\min },\tmspace  +\thickmuskip {.2777em} \beta _0^{\max },\tmspace  +\thickmuskip {.2777em} k$}}{28}{algorithm.1}\protected@file@percent }
\newlabel{alg:topogen-labels-sampling}{{1}{28}{Sampling $(\beta _0, g)$ \\ \textbf {Input:} $g^{\max },\; G^{\max },\; \beta _0^{\min },\; \beta _0^{\max },\; k$}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Choosing Components Shapes}{28}{subsection.A.3}\protected@file@percent }
\newlabel{ssec:suppl_choosing_components}{{A.3}{28}{Choosing Components Shapes}{subsection.A.3}{}}
\citation{counting}
\abx@aux@cite{0}{counting}
\abx@aux@segm{0}{0}{counting}
\newlabel{tab:topogen-hyperparams}{{\caption@xref {tab:topogen-hyperparams}{ on input line 93}}{29}{Role of $k$}{table.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Hyperparameter values used for the DONUT dataset for the experiments. Probing tasks boil down to classifying shapes across $\beta _0^{\max } - \beta _0^{\min } + 1 = 6$ categories for connected components and 11 categories for genera.}}{29}{table.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algorithmic Details.}{29}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Complexity.}{29}{subsection.A.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \textsc  {Enumerate-Solutions}   \textbf  {Input:} $a$, $b$, $g^{max}$   \textbf  {Output:} $S$}}{30}{algorithm.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces \textsc  {Backtrack}   \textbf  {Input:} $r_{\text  {count}}$, $r_{\text  {sum}}$, $k$, $\mathbf  {x}$, $S$   \textbf  {Output:} $S$}}{30}{algorithm.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Baselines Training Details}{30}{subsection.A.4}\protected@file@percent }
\newlabel{ssec:topogen-baseline-training}{{A.4}{30}{Baselines Training Details}{subsection.A.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Additional Results}{30}{subsection.A.5}\protected@file@percent }
\newlabel{ssec:topogen-additional-results}{{A.5}{30}{Additional Results}{subsection.A.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparison of typical training hyperparameters for point-based models (PointNet, PointNet++, DGCNN) and topology-based models (PersFormer, xPerT, PersLay). A vertical line separates the two families of architectures.}}{31}{table.caption.25}\protected@file@percent }
\newlabel{suppl:topogen-baseline-training}{{6}{31}{Comparison of typical training hyperparameters for point-based models (PointNet, PointNet++, DGCNN) and topology-based models (PersFormer, xPerT, PersLay). A vertical line separates the two families of architectures}{table.caption.25}{}}
\abx@aux@read@bbl@mdfivesum{C15F40BEFFA83DC56E658B4A9540B92A}
\abx@aux@defaultrefcontext{0}{persistence_images}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{intermediate_layers}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mantra}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{superquadrics}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{top_layer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{persistence_landscapes}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{top_signatures}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{optimizing_persistent_homology}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perslay}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shapenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{extended_persistence}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{objaverse}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{objaverse_xl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{superdec}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{eulearn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{topology_activations}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{superformula}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{betti_curves}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{plato}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{structural_probe}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{topological_signature}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{topology_aware_latent_diffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{manifold}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{manifoldplus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{platonic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pllay}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xpert}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{abc}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cka}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{set-transformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pointcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{prae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pvcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{topological_autoencoders}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{adaptive_topological_feature}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pmae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pointnet++}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pointnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shapellm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vit_probing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{persformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{atol}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{counting}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{smoothgrad}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kpconv}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{top_reg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{multiset_transformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dogn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dgcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{persistent_homology_seg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pointconv}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pointllm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ulip2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ulip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{neural_approximation_graph_topological_features}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hfbrimae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pbert}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clay}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pm2ae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pcpmae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{uni3d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{thingi}{nty/global//global/global/global}
\abx@aux@defaultlabelprefix{0}{persistence_images}{}
\abx@aux@defaultlabelprefix{0}{intermediate_layers}{}
\abx@aux@defaultlabelprefix{0}{mantra}{}
\abx@aux@defaultlabelprefix{0}{superquadrics}{}
\abx@aux@defaultlabelprefix{0}{top_layer}{}
\abx@aux@defaultlabelprefix{0}{persistence_landscapes}{}
\abx@aux@defaultlabelprefix{0}{top_signatures}{}
\abx@aux@defaultlabelprefix{0}{optimizing_persistent_homology}{}
\abx@aux@defaultlabelprefix{0}{perslay}{}
\abx@aux@defaultlabelprefix{0}{shapenet}{}
\abx@aux@defaultlabelprefix{0}{extended_persistence}{}
\abx@aux@defaultlabelprefix{0}{objaverse}{}
\abx@aux@defaultlabelprefix{0}{objaverse_xl}{}
\abx@aux@defaultlabelprefix{0}{superdec}{}
\abx@aux@defaultlabelprefix{0}{eulearn}{}
\abx@aux@defaultlabelprefix{0}{topology_activations}{}
\abx@aux@defaultlabelprefix{0}{superformula}{}
\abx@aux@defaultlabelprefix{0}{betti_curves}{}
\abx@aux@defaultlabelprefix{0}{plato}{}
\abx@aux@defaultlabelprefix{0}{structural_probe}{}
\abx@aux@defaultlabelprefix{0}{topological_signature}{}
\abx@aux@defaultlabelprefix{0}{topology_aware_latent_diffusion}{}
\abx@aux@defaultlabelprefix{0}{manifold}{}
\abx@aux@defaultlabelprefix{0}{manifoldplus}{}
\abx@aux@defaultlabelprefix{0}{platonic}{}
\abx@aux@defaultlabelprefix{0}{pllay}{}
\abx@aux@defaultlabelprefix{0}{xpert}{}
\abx@aux@defaultlabelprefix{0}{abc}{}
\abx@aux@defaultlabelprefix{0}{cka}{}
\abx@aux@defaultlabelprefix{0}{set-transformer}{}
\abx@aux@defaultlabelprefix{0}{pointcnn}{}
\abx@aux@defaultlabelprefix{0}{prae}{}
\abx@aux@defaultlabelprefix{0}{pvcnn}{}
\abx@aux@defaultlabelprefix{0}{topological_autoencoders}{}
\abx@aux@defaultlabelprefix{0}{adaptive_topological_feature}{}
\abx@aux@defaultlabelprefix{0}{pmae}{}
\abx@aux@defaultlabelprefix{0}{pointnet++}{}
\abx@aux@defaultlabelprefix{0}{pointnet}{}
\abx@aux@defaultlabelprefix{0}{shapellm}{}
\abx@aux@defaultlabelprefix{0}{vit_probing}{}
\abx@aux@defaultlabelprefix{0}{persformer}{}
\abx@aux@defaultlabelprefix{0}{atol}{}
\abx@aux@defaultlabelprefix{0}{counting}{}
\abx@aux@defaultlabelprefix{0}{smoothgrad}{}
\abx@aux@defaultlabelprefix{0}{kpconv}{}
\abx@aux@defaultlabelprefix{0}{top_reg}{}
\abx@aux@defaultlabelprefix{0}{multiset_transformer}{}
\abx@aux@defaultlabelprefix{0}{dogn}{}
\abx@aux@defaultlabelprefix{0}{dgcnn}{}
\abx@aux@defaultlabelprefix{0}{persistent_homology_seg}{}
\abx@aux@defaultlabelprefix{0}{pointconv}{}
\abx@aux@defaultlabelprefix{0}{pointllm}{}
\abx@aux@defaultlabelprefix{0}{ulip2}{}
\abx@aux@defaultlabelprefix{0}{ulip}{}
\abx@aux@defaultlabelprefix{0}{neural_approximation_graph_topological_features}{}
\abx@aux@defaultlabelprefix{0}{hfbrimae}{}
\abx@aux@defaultlabelprefix{0}{pbert}{}
\abx@aux@defaultlabelprefix{0}{clay}{}
\abx@aux@defaultlabelprefix{0}{pm2ae}{}
\abx@aux@defaultlabelprefix{0}{pcpmae}{}
\abx@aux@defaultlabelprefix{0}{uni3d}{}
\abx@aux@defaultlabelprefix{0}{thingi}{}
\gdef \@abspage@last{35}
