\begin{abstract}
\label{sec:abstract}
Large transformer-based foundation models have shown remarkable ability to extract expressive representations in a purely self-supervised, data-driven manner, and recent efforts have extended them to 3D shape understanding. In parallel, Topological Data Analysis (TDA), through the lens of persistent homology, offers mathematically grounded tools to characterize the structural information carried by point clouds. However, persistent homology scales poorly with dimension and sample size, limiting its applicability to large datasets. This thesis makes two main contributions. First, we investigate to what extent pretrained 3D transformer models capture structural, non-semantic properties of shapes, using TDA as a principled framework to quantify their topological awareness. Second, building on these insights, we introduce a data-driven proxy for persistent homology: a transformer-based approach that leverages learned representations to predict topological features efficiently. Our results highlight both the limitations of current 3D transformers and the potential of bridging foundation models with TDA to advance topology-aware 3D representation learning.
% Transformers have become the dominant architecture for representation learning across modalities, achieving strong performance in natural language processing and computer vision. This has motivated recent efforts to extend transformers to 3D shape understanding. While these models show promising results, they face two fundamental challenges. First, transformers are notoriously data-hungry, yet even the largest 3D shape datasets remain orders of magnitude smaller than those in text or vision. Second, although transformers perform well on simple benchmarks, they struggle with tasks that require reasoning about complex geometries and topologies. In particular, it is unclear whether pretrained 3D transformer models capture the structural, non-semantic properties of shapes, beyond surface-level geometric or categorical information. In this work, we systematically investigate this question. We demonstrate that current 3D transformers are intrinsically limited in their ability to capture such structural information, and we introduce a new procedure (tbd) that addresses this limitation and improves their capacity to represent challenging 3D shape properties.
\end{abstract}