\section{DONUT: \underline{D}ataset \underline{O}f ma\underline{N}ifold str\underline{U}c\underline{T}ures}
\label{sec:topogen}

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{figs/topogen/samples_overview.png}

  \label{fig:topogen-samples}
  \caption{\textbf{Random samples from DONUT.} Despite the fact that each sample is generated from a rather small family of shapes, both the topology preserving placement and augmentations allow for a rich variety of shapes, while ensuring topological consistency.}
  \label{fig:short}
\end{figure}


Understanding how models capture topological properties of 3D shapes is a key step toward disentangling geometric representation learning from semantic categorization. Existing 3D datasets, such as Objaverse~\cite{objaverse,objaverse_xl} or ShapeNet~\cite{shapenet}, primarily organize data by semantic class or surface geometry, without systematic control of topology. Probing topology-aware representations therefore requires datasets that expose explicit topological labels under controlled geometric variability.

To address this, we introduce DONUT, a scalable dataset of synthetic 3D shapes annotated with precise and balanced topological labels. Unlike prior datasets, DONUT is built from parametric families that guarantee topological correctness while enabling high geometric diversity. This design allows principled evaluation of how models represent intrinsic topological invariants such as connected components and genus.

\subsection{Existing datasets}
\label{ssec:existing_datasets}

Existing 3D datasets provide only limited support for probing whether models capture topological structure. Broadly, they fall into three categories: combinatorial-only benchmarks, geometric datasets with noisy topology, and synthetic datasets with limited diversity.

\paragraph{Combinatorial benchmarks.}
The MANTRA dataset (Manifold Triangulation Assemblage) \cite{mantra} provides combinatorial triangulations of 2D and 3D manifolds, represented as abstract simplicial complexes without embedding in $\mathbb{R}^3$. MANTRA is a valuable testbed for assessing whether graph- or simplicial complex models capture higher-order structures such as Betti numbers $(\beta_0, \beta_1, \beta_2)$. However, since it lacks any geometric realization, MANTRA is not suitable for evaluating models built on geometric 3D representations such as meshes, point clouds, or implicit fields.

\input{tables/dataset-compare.tex}

\paragraph{Noisy topology.}
Large mesh datasets such as Thingi10K~\cite{thingi}, Objaverse~\cite{objaverse} and ABC~\cite{abc} contain CAD models and artistic objects, and include coarse topological annotations (number of components, genus). However, these annotations are unreliable for several reasons:

\begin{enumerate}
  \item \textit{Discrepancy between raw and perceptual components.} Artistic and CAD models are typically constructed from many sub-meshes, so the annotated component count often diverges from the semantically meaningful object count.
  \item \textit{Severe class imbalance.} While a wide range of genus values is theoretically possible, the overwhelming majority of meshes in both datasets have genus 0â€“2, making them unsuitable for balanced probing tasks.
  \item \textit{Structural artifacts.} Many meshes are non-manifold or self-intersecting, rendering quantities such as the Euler characteristic ill-defined:
\begin{equation}
  \chi = V - E + F = 2 - 2g - b + c
  \label{eq:euler}
\end{equation}

where $V,E,F$ are the number of vertices, edges, and faces, $g$ is genus, $b$ the number of boundary components, and $c$ the number of connected components.
Attempts to repair such meshes (e.g., Manifold~\cite{manifold}, ManifoldPlus~\cite{manifoldplus}, DOGN~\cite{dogn}, CLAY~\cite{clay}) face tradeoffs between oversmoothing geometry, introducing artifacts, or incurring prohibitive computational cost. As a result, these datasets cannot provide reliable large-scale topological ground truth.
\end{enumerate}

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.40\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/topogen/thingi_genus_hist.pdf}
    \caption{\textbf{Genus distribution of Thingi10K.} Both axes are in log scale. The genus, estimated from the Euler characteristic provided as metadata, could not be computed for 2,651 samples, which are excluded. The histogram shows that most meshes (over 3,000 of 7,344) have genus 0 or 1, highlighting the scarcity of higher-genus shapes.}
    \label{fig:thingi-genus}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.58\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/connected_comps_thingi.pdf}
    \caption{\textbf{Raw connected components.} The left figure shows a mesh made of 8 connected components, highlighted with different colors. However, once converted into a point cloud (right figure), we lose track of this information. Therefore, labeling this sample as having 8 connected components would be misleading for a model.}
    \label{fig:thingi-comps}
  \end{subfigure}
  \caption{}
  \label{fig:thingi-overview}
\end{figure}

\paragraph{Synthetic datasets with limited diversity.} 
The EuLearn dataset~\cite{eulearn} addresses class imbalance by generating surfaces of varying genus from Fourier curves. While balanced across genera, EuLearn suffers from two critical issues: (i) samples within each genus class lack diversity, making them nearly indistinguishable, and (ii) strong correlations emerge between genus and canonical orientation, enabling trivial classification via nearest-neighbor with Chamfer distance. Consequently, EuLearn fails to disentangle topological structure from geometric shortcuts, limiting its usefulness for robust probing.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.5\linewidth]{figs/topogen/thingi_genus_hist.pdf}
%    \caption{\textbf{Genus distribution of Thingi10K.} Both axes are plotted in log scale. The genus was estimated from the Euler characteristic, provided as metadata with the dataset; however, 2,651 samples do not fulfill the requirements to directly compute the genus from the Euler characteristic (see \ref{eq:euler}). They are therefore not taken into account here. The histogram shows that a large fraction of the dataset (over 3,000 samples out of 7,344) has genera of 0 or 1, indicating that higher-genus components are significantly underrepresented, which may limit accurate classification and probing analyses for those cases.}
%    \label{fig:thingi-genus}
% \end{figure}




\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{figs/eulearn/dataset_samples.pdf}
   \caption{\textbf{Samples from the EuLearn dataset across different genera.} As the genus increases, shapes become geometrically more complex. This trend highlights a confounding factor in the dataset: geometric complexity grows together with genus.}
   \label{fig:eulearn-samples}
\end{figure}


\subsection{Method}
\label{ssec:topogen-method}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/topogen/topogen_gen.pdf}
   \caption{\textbf{Generation Process Overview.} (1) For each sample, we pick a target genus and number of connected components. (Section~\ref{sssec:labels-distribution}) (2) We then distribute these values across a set of template shapes (Section~\ref{sssec:sample-level-properties}). (3) Each mesh is instantiated by sampling parameters from its parametric expression, and individually warped to bring more geometric diversity. (4) Finally, we apply a series of topology-preserving augmentations to increase geometric diversity while maintaining the original topological labels, and compose the final sample by merging all meshes. (Section~\ref{sssec:mesh-generation})}
   \label{fig:topogen-overview}
\end{figure}

DONUT is a synthetic dataset designed to study how models capture topology under controlled conditions. Building such a dataset raises two challenges. First, synthetic data may differ from real 3D shapes used to pretrain large models, which could make it out-of-distribution. Second, synthetic datasets often lack geometric diversity, making probing tasks trivial.

The first issue is minor because most pretrained encoders (e.g., trained on ShapeNet or Objaverse) already include synthetic geometry. Figure~\ref{fig:topogen-umaps-overview} confirms that DONUT samples project into the same embedding space as these datasets, showing no strong distribution shift. The second issue is addressed through a generation pipeline that jointly ensures (i) exact topological control, (ii) high geometric diversity, and (iii) scalability. Figure \ref{fig:topogen-overview} summarizes the process.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figs/topogen/umaps_overview.pdf}
  \caption{\textbf{UMAP embeddings of features learned by 3D encoders.}}
  \label{fig:topogen-umaps-overview}
\end{figure}


\subsubsection{Topological Label Sampling}
\label{sssec:labels-distribution}

To ensure balanced supervision across all classes, both the genus and the number of connected components are sampled such that their marginal distributions are approximately uniform. Appendix~\ref{sec:suppl_topogen} formally describes how the labels are sampled and further details the properties of the dataset we used for all our experiments.

\subsubsection{Shape Instantiation}
\label{sssec:sample-level-properties}

Within a sample, each component is instantiated from a parametric family chosen according to its target genus:
\begin{itemize}
  \item \textit{Genus 0:} Superellipsoids and cones
  \item \textit{Genus 1:} Supertoroids
  \item \textit{Genus $\geq 2$:} K-tori
\end{itemize}

\textbf{Superquadrics.} Superellipsoids  (resp. toroids) are part of a wider family of parametric shapes called superquadrics. They were introduced by Barr et al. \cite{superquadrics} in 1981 and are widely used in computer graphics for their ability to represent a large range of shapes with a small number of compact parameters. Superquadrics have recently regained attention in 3D scene understanding, notably in SuperDec \cite{superdec}, because of their expressiveness and their ability to approximate complex structures by composition. Their implicit equation is given by:

\begin{equation}
\left( \left| \frac{x}{s_x} \right|^{\tfrac{2}{\epsilon_2}}
     + \left| \frac{y}{s_y} \right|^{\tfrac{2}{\epsilon_2}} \right)^{\tfrac{\epsilon_2}{\epsilon_1}}
+ \left| \frac{z}{s_z} \right|^{\tfrac{2}{\epsilon_1}}
= 1
\end{equation}

where $(s_x, s_y, s_z) > 0$ are scale factors along the $x, y, z$ axes respectively, and $(\epsilon_1, \epsilon_2) > 0$ are shape exponents that modulate the surface's roundness (resp. sharpness). Sampling these parameters within predefined ranges allows to efficiently sample a variety of shapes while maintaining control over their topological properties (see Figure~\ref{fig:overview}).

\subsubsection{Meshes Generation}
\label{sssec:mesh-generation}

To generate many samples (up to $10^5$) in a reasonable amount of time, we leverage the parametric forms of the shapes whenever possible. This approach minimizes computationally intensive operations and allows for efficient mesh generation.

\paragraph{Cones.} To be completed

\paragraph{Superquadrics.} Meshes corresponding to super ellipsoids and super toroids are generated directly from their parametric forms. The procedure consists of two steps: 1) generate a base template mesh (a sphere for ellipsoids, a torus for toroids) where vertices are expressed in spherical (resp. toroidal) coordinates, and 2) deform it according to the superquadric equations. This construction is lightweight, parallelizable, and supports fast large-scale dataset generation. The parametric equations we use in practice are as follows:
\begin{equation}
\begin{aligned}
\text{Ellipsoid} \quad
\begin{cases}
x(u,v) &= s_x \, C_{\epsilon_1}(v) \, C_{\epsilon_2}(u) \\
y(u,v) &= s_y \, S_{\epsilon_1}(v) \, S_{\epsilon_2}(u) \\
z(u,v) &= s_z \, S_{\epsilon_1}(v)
\end{cases}
\end{aligned}
\qquad
\begin{aligned}
\text{Toroid} \quad
\begin{cases}
x(u,v) &= s_x \, \bigl(R + C_{\epsilon_1}(v)\bigr) \, C_{\epsilon_2}(u) \\
y(u,v) &= s_y \, \bigl(R + S_{\epsilon_1}(v)\bigr) \, S_{\epsilon_2}(u) \\
z(u,v) &= s_z \, S_{\epsilon_1}(v)
\end{cases}
\end{aligned}
\end{equation}


where $u,v \in [-\pi, \pi]$ are the vertex coordinates of the template mesh is spherical (resp. toroidal) coordinates and:

\begin{equation}
\begin{aligned}
C_\epsilon(u) &= \operatorname{sign}(\cos(u)) \, |\cos(u)|^\epsilon \\
S_\epsilon(u) &= \operatorname{sign}(\sin(u)) \, |\sin(u)|^\epsilon
\end{aligned}
\end{equation}


\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/topogen/toroids_overview.png}
    \caption{Supertoroids.}
    \label{fig:toroids-overview}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/topogen/ellipsoids_overview.png}
    \caption{Superellipsoids.}
    \label{fig:ellipsoids-overview}
  \end{subfigure}
  \caption{Overview of different shapes obtained for fixed values of $a_i, i \in \{1, 2, 3\}$ and increasing values of $\epsilon_1$ (left to right) and $\epsilon_2$ (bottom to top). 
  (a) Different supertoroids. As mentioned in TO ADD, using these shapes for $k$-tori ($k \geq 2$) is challenging because they may not preserve the genus, for instance if some parts are too thin or sharp. 
  (b) Different superellipsoids.}
  \label{fig:overview}
\end{figure}

\paragraph{$K$-tori.} In the case of higher-genus meshes (Figure~\ref{fig:ktori-overview}), there are no closed-form parametric equations. Instead, we leverage the implicit formulation of tori. A $k$-torus is generated by (1) evenly distributing tori on a circle, (2) blending their signed distance functions (SDF) through a smooth union operator, and finally (3) extracting the mesh via marching cubes with a grid-size. However, this process is more computationally intensive as it requires applying marching cubes the discretized SDF, whose complexity scales in $O(N^3)$ where $N$ is the grid size. And since we seek to have high quality meshes to preserve fine-grained details (here holes), we use a higher grid resolution.

To smoothly blend the SDF of each torus generated independently, we use the \textit{smooth minimum} operator:

\begin{equation}
\operatorname{softmin}_k(s_1, s_2, \dots, s_n) 
= -\frac{1}{k} \log \left( \sum_{i=1}^n e^{-k s_i} \right)
\end{equation}

Where $(s_1, s_2, \dots, s_n) \in \mathbb{R}^{(N^3)}$ are the SDF of individual tori and $k$ is a hyperparameter controlling the smoothness of the blending. This operator is applied voxel wise. In other words, each voxel of the blended SDF is assigned with the smooth minimum value of the individual SDF.

\subsubsection{Topological Consistency and Diversity}
\label{sssec:top-consistency}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figs/topogen/placement.pdf}
  \caption{\textbf{Component placement procedure.} (1) A new component is randomly placed within the sample. (2) If it overlaps with existing components, we identify the point on the new mesh that is most deeply inside the other component and push the new component along the opposite direction of the surface normal at that point. This step is repeated up to five times. If overlaps remain, the component is discarded and a new one is generated and randomly placed. This procedure ensures that components do not overlap while remaining close enough to preserve topological consistency and challenge models to correctly identify the number of connected components.}
  \label{fig:topogen-placement}
\end{figure}

Two challenges remain to be addressed. First, when placing components within a sample, we must ensure they don't overlap. At the same time, they need to be close enough to make the samples both topologically consistent and challenging enough for models to correctly identify the number of connected components. In early experiments, we observed that if components are too far apart, the task can be trivially solved using a simple KNN classifier with Chamfer distance. Second, even with careful placement, models tend to overfit, sometimes even when using simple linear heads on top of learned features, showing that this alone is insufficient to evaluate their true topological understanding. To increase variability while preserving topological labels, we apply transformations both at the sample level and the individual component level.

\paragraph{Topological Consistency.} 
We developed a two-stage placement procedure. Suppose there are already some components correctly placed within a sample. To add a new component, we first place it randomly in the sample and then check for intersections with the existing components. Experimentally, we observed that a randomly placed mesh overlaps with at most two other components. If the new component overlaps with only one existing component, we identify the point on the new mesh that is most deeply inside the other component and push the new component along the opposite direction of the surface normal at that point. For intricate shapes, a single adjustment may create new overlaps, so we repeat this step five times (see Figure~\ref{fig:topogen-placement}). If overlaps remain after five attempts, the component is discarded and a new one is generated and randomly placed. This iterative procedure improves efficiency: in practice, one or two adjustments are sufficient to resolve overlaps, making it faster than discarding and regenerating components immediately.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/topogen/ktori_overview.png}
    \caption{\textbf{Degrees of freedom allowed on $k$-tori.} 
    Prior to the sequence of transformations applied to each individual component (e.g. rotation, twisting) we allow some variability during the generation. 
    The x-axis represents the ratio \textit{major radius / minor radius}. 
    The y-axis shows how the template shape is modified as we increase the value of $k$ (i.e. the genus). 
    \textit{Note:} In the actual dataset, $1$-tori are generated with the parametric representation of supertoroids, since a regular torus is a particular case of supertoroid. 
    However, for $k \geq 2$, using a composition of $1$-tori is more reliable. 
    Some parameter sets can lead to undesirable holes in the final shape.}
    \label{fig:ktori-overview}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/topogen/twisted_comparison.png}
    \caption{\textbf{Effect of twisting.} 
    Original template shapes (left) are twisted along the purple axis (right). 
    Twisting deformations introduce non-rigid variability while preserving the genus and connectivity of each component. 
    Here the scalar function defined along the axis is affine between $-\pi/6$ and $\pi/3$. 
    We apply this augmentation at the component and sample level.}
    \label{fig:twisted-comparison}
  \end{subfigure}
\end{figure}

\paragraph{Variability.} 
We apply transformations both at the sample level and at the component level. Commonly used rigid transformations, such as rotations and scaling, are applied to both components and full samples. At the component level, we do not apply translations to avoid creating overlaps that would break the overall topology of the sample. To introduce additional geometric variability, we also apply twisting deformations (Figure \ref{fig:twisted-comparison}). Without loss of generality, we assume that a component (or a full sample) lies within the unit sphere, as global scaling can be applied afterward. First, we uniformly sample a direction on the unit sphere, which defines an axis for the twist. Next, we define a smooth scalar function along this axis that determines the rotation angle. Finally, each vertex is rotated around the axis by the angle given by the scalar function evaluated at the point where the vertex projects along the axis. This procedure introduces non-rigid variability while preserving the genus and connectivity of each component and of the overall sample.

\subsection{Analysis}

This section provides a detailed characterization of DONUT. We first present dataset statistics (Section~\ref{sssec:topogen-general-properties}) and label distributions. We then assess baseline performance and introduce two complementary analyses to ensure models trained on DONUT truly learn the intended topological signal rather than relying on shortcuts. The first (Section~\ref{sssec:topogen-transferability}) measures performance drop when models trained on DONUT are evaluated on a distinct dataset. The second (Section~\ref{sssec:topogen-saliency}), visualizes model attention on template shapes to reveal which regions influence predictions.

\subsubsection{General Properties}
\label{sssec:topogen-general-properties}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figs/topogen/label_distribution.pdf}
  \caption{\textbf{Label distributions for genus and connected components.} The marginal distributions of genus and connected components (left and middle histograms) are approximately uniform, ensuring balanced supervision across topological classes. The right plot shows the joint distribution. The absence of samples with one connected component and genus greater than 5 stems from the constraints of our generation process, which limits individual components to a maximum genus of 5. The only way to achieve a genus greater than 5 is by combining multiple components.}
  \label{fig:topogen-properties}
\end{figure}


\paragraph{Dataset statistics.}
DONUT contains 29,517 samples divided into training, validation, and test splits of 80\%, 10\%, and 10\%. Each sample has between 1 and 6 connected components, with the total genus ranging from 0 to 10. A single component has genus at most 5 (corresponding to a 5-torus (Figure~\ref{fig:ktori-overview})). The dataset size was chosen to balance feasibility and reliability: smaller datasets led to overfitting while much larger datasets made experiments costly. Figure~\ref{fig:topogen-properties} reports the marginal distributions of genera and connected components.

\paragraph{Distributions and scalability.}
Figure~\ref{fig:topogen-umaps-overview} shows that synthetic samples from DONUT occupy the same representation space as training data used by common 3D encoders. This indicates that the dataset lies within the learned distribution of existing models rather than being out-of-distribution. In addition, Figure~\ref{fig:topogen-gen-time} reports the time required to generate different dataset sizes, showing that large-scale variants remain practical.


\subsubsection{Baselines and Transferability}
\label{sssec:topogen-transferability}

We trained several classical point-cloud models (PointNet, PointNet++, DGCNN) and recent persistence-based models (PersFormer, xPerT, PersLay) on DONUT. Results are reported in Table~\ref{tab:topogen-results}. Beyond in-distribution accuracy, a key question is whether these models actually capture topology or only rely on geometric characteristics.

Transferability provides a natural probe. To test this, we evaluated models on a curated subset of ABC, without fine-tuning. This set contains shapes whose geometry differs significantly from the training distribution, but whose topology is well characterized and within the label space. If models have learned topology, they should retain reasonable performance on ABC despite geometric differences.

\paragraph{Results.}
Table~\ref{tab:topogen-results} shows that DGCNN and PointNet++ achieve competitive results on predicting the number of connected components of DONUT. However, it also highlights that raw knowledge transfer, without fine-tuning, to ABC is very limited. It suggests that models remain highly sensitive to geometric distribution shifts, even when the task is purely topological.

\input{tables/donut-baselines.tex}


\subsubsection{Saliency Analysis}
\label{sssec:topogen-saliency}

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/topogen/saliencies.pdf}
   \caption{\textbf{Saliency maps of toy shapes with varying topology.} We show saliency for DGCNN as it's the best performing model on DONUT (Table~\ref{tab:topogen-results}). Top row shows saliency maps on genus prediction for samples with a single connected component and increasing genus. We also specify the genus predicted by the model. Bottom row highlights how saliency maps change between genus (left) and connected component (right) prediction for the same sample with two connected components.}
   \label{fig:topogen-saliencies}
\end{figure}

To further understand model behavior, we visualize in Figure~\ref{fig:topogen-saliencies} saliency maps on toy shapes. These maps highlight which regions contribute most to predictions. When trained to predict genus, models tend to focus on cycles, while for connected components they concentrate around contact points between shapes. Such results suggest that models attend to meaningful topological structures rather than exploiting geometric shortcuts.

\paragraph{Definition.}
Let a point cloud be $X=[x_1,\dots,x_N] \in \mathbb{R}^{N\times 3}$. Let $f_\theta(X)\in\mathbb{R}^C$ be the vector of class logits for $C$ classes. The saliency of point $x_i$ for class $c$ is the norm of the gradient of the class score with respect to the point coordinates:

\begin{equation}
  S_{i,c} = \left\| \frac{\partial f_\theta(X)_c}{\partial x_i} \right\|_2
\end{equation}

\paragraph{Interpretation on point clouds.}
Saliency quantifies which points most influence the class score under small coordinate changes. Points with larger $S_{i,c}$ contribute more to the decision, because an infinitesimal displacement of those coordinates changes $f_\theta(X)_c$ the most. In our topological setting this yields a clear reading. For genus prediction, high saliency concentrates along cycles whose perturbation would alter the number of one dimensional holes. For connected components, high saliency appears near thin links and contact regions where minor displacements can merge or split components. These observations are consistent with prior work that adapts saliency to point clouds to estimate pointwise importance for recognition.

\paragraph{Practical computation.}
We backpropagate the gradient of the selected logit with respect to the input coordinates and compute $S_{i,c}$ for all points. Raw gradients can be noisy. Before visualization we therefore post-process raw maps by adapting~\cite{smoothgrad}. Concretely, we clip $S_{i,c}$ at the $\tau$-th percentile $q_\tau$ to suppress extreme outliers, then scale by $\tilde{S_{i,c}} = \min(S_{i,c},q_\tau)/q_tau \in [0,1]$. . We set all values below a small fraction of the maximum to zero to improve visual clarity. Smoothing strategies such as averaging saliency over noisy perturbations of the input are known to reduce noise further, but we keep our pipeline simple and report thresholded and normalized maps.

\paragraph{Results.}
Figure~\ref{fig:topogen-saliencies} highlights that when the prediction is correct, saliency sometimes captures the expected topological structures, as seen in the top row for genus 2 and 5 shapes. However, even for successful predictions, interpretation is not always straightforward. The points that contribute most to the prediction do not always follow a clear or consistent spatial pattern. When the model fails, as for genus 4, the saliency distribution becomes even less interpretable.

The second row of Figure~\ref{fig:topogen-saliencies} provides more insight. For genus prediction, the salient points tend to cluster around the main topological structures, while for component prediction, they concentrate near the contact region between connected components. This suggests that the model distinguishes between tasks by attending to different geometric cues.

It is also worth noting that the saliency patterns vary with rotation, scale, and the prominence of the topological structures themselves, such as the size of a hole or the distance between components. \textit{These observations indicate that the modelâ€™s predictions remain strongly influenced by geometric properties, even when the task requires reasoning about topology.}

% \begin{figure}[t]
%   \centering
%   \begin{subfigure}[t]{0.66\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figs/topogen/components_genus_hist.pdf}
%     \caption{Label distributions for genus and connected components.}
%     \label{fig:topogen-gen-comp-hist}
%   \end{subfigure}%
%   \hfill
%   \begin{subfigure}[t]{0.33\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figs/topogen/generation_time.pdf}
%     \caption{Scalability of dataset generation.}
%     \label{fig:topogen-gen-time}
%   \end{subfigure}
%   \caption{\textbf{General properties of DONUT.} \textit{(a),(b)} The marginal distributions of genus and connected components are approximately uniform, ensuring balanced supervision across topological classes. \textit{(c)} The time required to generate datasets of varying sizes demonstrates that even large-scale variants remain practical.}
%   \label{fig:topogen-properties}
% \end{figure}


\subsection{Limitations and Further Improvements}

\paragraph{Topological Variety.} 
The current dataset has limited topological variety. It only encodes two characteristics: genus and number of connected components. This is mainly because all meshes are watertight and manifold. In this setting, topological invariants are trivial to deduce ($\beta_1= 2 \times \text("genus")$, $\beta_2=\beta_0$). A more challenging extension would include surfaces with boundaries, non-watertight meshes, or nested shapes. While the current generation process allows nested components in theory, no such cases are present in practice. Another direction is to incorporate families of parametric shapes with non-trivial topology, such as catenoids, helicoids, or MÃ¶bius strips.


\paragraph{Geometric Complexity.} The geometric diversity is also limited. It could be extended by adding parametric families that introduce richer geometric structures. For example, the Superformula introduced by Gielis (2003) \cite{superformula} generalizes superellipses and provides a broad range of shapes. Although we did not add such families here, since the dataset is mainly used for evaluation and no overfitting issues were observed with the current configuration, this would be valuable in other settings. In particular, if the dataset is used for pretraining, as in Point-MAE-Zero, introducing more geometric complexity would improve its utility.


