\section{Conclusion}

This work investigated whether modern transformer-based 3D encoders capture the topological structure of shapes. Using DONUT, a controlled benchmark we specifically designed for this purpose, we were able to quantify how much these models understand intrinsic topological features such as connected components and genus. Across all analyses and architectures, results consistently showed that current 3D transformers lack a clear understanding of topology. Their representations reflect geometric and semantic cues but fail to encode structural invariants that define the global organization of a shape.

This work investigated whether modern transformer-based 3D encoders capture the topological structure of shapes. Using DONUT, a controlled benchmark we specifically designed for this purpose, we were able to quantify how much these models understand intrinsic topological features such as connected components and genus. Across all analyses and architectures, results consistently showed that current 3D transformers lack a clear understanding of topology. Their representations reflect geometric and semantic cues but fail to encode structural invariants that define the global organization of a shape.

These findings point to a broader limitation of current 3D representation learning approaches. When moving beyond curated benchmarks to real-world data, we often encounter shapes that do not have clear semantic labels or stable geometric parametrizations but share common topological patterns. In such cases, models that ignore topology risk missing essential structural information. Bridging topological data analysis with deep learning, as explored here, offers a principled way to address this gap and build representations that capture not only what a shape looks like but also how it is fundamentally organized.